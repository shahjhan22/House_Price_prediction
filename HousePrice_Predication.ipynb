{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eec0066c-b239-485f-9855-82fc4baca9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "571ed1ae-855f-4601-8529-71908740d4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"houseprice.csv\")\n",
    "pd.set_option('display.max_columns',None)\n",
    "df.head(2)\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e714ac8-3364-429b-86e6-6c37679fa21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1201\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df=pd.read_csv(\"houseprice.csv\",usecols=['MSSubClass','MSZoning','LotFrontage','LotArea','LotConfig','OverallQual','OverallCond','YearBuilt','SalePrice','1stFlrSF','2ndFlrSF','LotShape']).dropna()\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fadcc0b-f9d5-4dd3-a8ae-3bb34192a26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1201 entries, 0 to 1459\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   MSSubClass   1201 non-null   int64  \n",
      " 1   MSZoning     1201 non-null   object \n",
      " 2   LotFrontage  1201 non-null   float64\n",
      " 3   LotArea      1201 non-null   int64  \n",
      " 4   LotShape     1201 non-null   object \n",
      " 5   LotConfig    1201 non-null   object \n",
      " 6   OverallQual  1201 non-null   int64  \n",
      " 7   OverallCond  1201 non-null   int64  \n",
      " 8   YearBuilt    1201 non-null   int64  \n",
      " 9   1stFlrSF     1201 non-null   int64  \n",
      " 10  2ndFlrSF     1201 non-null   int64  \n",
      " 11  SalePrice    1201 non-null   int64  \n",
      "dtypes: float64(1), int64(8), object(3)\n",
      "memory usage: 122.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4904fdb-20e9-42ff-9d35-d7f3e95ac788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column name MSSubClass and unique value 15\n",
      "Column name MSZoning and unique value 5\n",
      "Column name LotFrontage and unique value 110\n",
      "Column name LotArea and unique value 869\n",
      "Column name LotShape and unique value 4\n",
      "Column name LotConfig and unique value 5\n",
      "Column name OverallQual and unique value 10\n",
      "Column name OverallCond and unique value 8\n",
      "Column name YearBuilt and unique value 112\n",
      "Column name 1stFlrSF and unique value 678\n",
      "Column name 2ndFlrSF and unique value 368\n",
      "Column name SalePrice and unique value 597\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns:\n",
    "    print(\"Column name {} and unique value {}\".format(i,len(df[i].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c04fe6-ef42-4643-993a-24ee149386d5",
   "metadata": {},
   "source": [
    "##### Converting categorical value to numerical using embadding , column are:MSSubClass,MSZoning,LotShape ,LotConfig,OverallQual,OverallCond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35e828b9-6bde-4ebb-baa9-34ed17fef76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "df['Total_years']=datetime.datetime.now().year-df['YearBuilt']   # derive column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cdf468f-199f-404b-b4e9-029c676c9b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['YearBuilt'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "164f9786-6092-42ea-b294-ae22f4795e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>Total_years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Inside</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>208500</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Reg</td>\n",
       "      <td>FR2</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>181500</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>223500</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Corner</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>140000</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>IR1</td>\n",
       "      <td>FR2</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>250000</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50</td>\n",
       "      <td>RL</td>\n",
       "      <td>85.0</td>\n",
       "      <td>14115</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Inside</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>796</td>\n",
       "      <td>566</td>\n",
       "      <td>143000</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>75.0</td>\n",
       "      <td>10084</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Inside</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1694</td>\n",
       "      <td>0</td>\n",
       "      <td>307000</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass MSZoning  LotFrontage  LotArea LotShape LotConfig  OverallQual  \\\n",
       "0          60       RL         65.0     8450      Reg    Inside            7   \n",
       "1          20       RL         80.0     9600      Reg       FR2            6   \n",
       "2          60       RL         68.0    11250      IR1    Inside            7   \n",
       "3          70       RL         60.0     9550      IR1    Corner            7   \n",
       "4          60       RL         84.0    14260      IR1       FR2            8   \n",
       "5          50       RL         85.0    14115      IR1    Inside            5   \n",
       "6          20       RL         75.0    10084      Reg    Inside            8   \n",
       "\n",
       "   OverallCond  1stFlrSF  2ndFlrSF  SalePrice  Total_years  \n",
       "0            5       856       854     208500           22  \n",
       "1            8      1262         0     181500           49  \n",
       "2            5       920       866     223500           24  \n",
       "3            5       961       756     140000          110  \n",
       "4            5      1145      1053     250000           25  \n",
       "5            5       796       566     143000           32  \n",
       "6            5      1694         0     307000           21  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0eba5ecd-0221-4cbb-8558-e07b32b0109e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>Total_years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>208500</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>181500</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>223500</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>140000</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>250000</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>85.0</td>\n",
       "      <td>14115</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>796</td>\n",
       "      <td>566</td>\n",
       "      <td>143000</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>75.0</td>\n",
       "      <td>10084</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1694</td>\n",
       "      <td>0</td>\n",
       "      <td>307000</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  MSZoning  LotFrontage  LotArea  LotShape  LotConfig  \\\n",
       "0           5         3         65.0     8450         3          4   \n",
       "1           0         3         80.0     9600         3          2   \n",
       "2           5         3         68.0    11250         0          4   \n",
       "3           6         3         60.0     9550         0          0   \n",
       "4           5         3         84.0    14260         0          2   \n",
       "5           4         3         85.0    14115         0          4   \n",
       "6           0         3         75.0    10084         3          4   \n",
       "\n",
       "   OverallQual  OverallCond  1stFlrSF  2ndFlrSF  SalePrice  Total_years  \n",
       "0            6            3       856       854     208500           22  \n",
       "1            5            6      1262         0     181500           49  \n",
       "2            6            3       920       866     223500           24  \n",
       "3            6            3       961       756     140000          110  \n",
       "4            7            3      1145      1053     250000           25  \n",
       "5            4            3       796       566     143000           32  \n",
       "6            7            3      1694         0     307000           21  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Label Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "cat_feature=['MSSubClass','MSZoning','LotShape','LotConfig','OverallQual','OverallCond']\n",
    "lbl_encoder={}\n",
    "for col in cat_feature:\n",
    "    lbl_encoder[col]=LabelEncoder()\n",
    "    df[col]=lbl_encoder[col].fit_transform(df[col])\n",
    "df.head(7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "275894b7-5d8c-47f9-ad80-799607565610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 3, 3, 4, 6, 3],\n",
       "       [0, 3, 3, 2, 5, 6],\n",
       "       [5, 3, 0, 4, 6, 3],\n",
       "       ...,\n",
       "       [6, 3, 3, 4, 6, 7],\n",
       "       [0, 3, 3, 4, 4, 4],\n",
       "       [0, 3, 3, 4, 4, 4]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_feature=np.stack([df['MSSubClass'],df['MSZoning'],df['LotShape'],df['LotConfig'],df['OverallQual'],df['OverallCond']],1)\n",
    "cat_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b96e752-1920-4793-80c9-0f96b4c989b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feature=torch.tensor(cat_feature,dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1bb0784-fcb3-4ab5-b4d8-51f944d8675c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LotFrontage', 'LotArea', '1stFlrSF', '2ndFlrSF', 'Total_years']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  for continuous variable\n",
    "continuous_feature=[]\n",
    "for col in df.columns:\n",
    "    if col not in ['MSSubClass','MSZoning','LotShape','LotConfig','OverallQual','OverallCond','SalePrice']:\n",
    "        continuous_feature.append(col)\n",
    "continuous_feature\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5ef10c76-b81b-49be-859d-aea49529bdbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   65.,  8450.,   856.,   854.,    22.],\n",
       "        [   80.,  9600.,  1262.,     0.,    49.],\n",
       "        [   68., 11250.,   920.,   866.,    24.],\n",
       "        ...,\n",
       "        [   66.,  9042.,  1188.,  1152.,    84.],\n",
       "        [   68.,  9717.,  1078.,     0.,    75.],\n",
       "        [   75.,  9937.,  1256.,     0.,    60.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stacking and converting into tensor\n",
    "continous_val=np.stack([df[i].values for i in continuous_feature],axis=1)\n",
    "continous_val=torch.tensor(continous_val,dtype=torch.float32)\n",
    "continous_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13d32e2f-db6e-4ed9-ab17-83bc2e4ebe68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([208500., 181500., 223500.,  ..., 266500., 142125., 147500.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=torch.tensor(df['SalePrice'],dtype=torch.float)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27415506-ea4e-4f60-9a8c-b184438507f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1201, 6]), torch.Size([1201, 5]), torch.Size([1201]), (1201, 12))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_feature.shape,continous_val.shape,y.shape,df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed698b04-6ebd-4318-8ad0-ed69c2baeab4",
   "metadata": {},
   "source": [
    "### Embedding  for categorical columns\n",
    "\n",
    "Embeddings are a powerful alternative to Label Encoding and One-Hot Encoding, especially when dealing with large categorical variables in deep learning models.\n",
    "\n",
    "### Label encoding problem\n",
    " Problem: The model may assume order (e.g., New York < Los Angeles), which is incorrect.\n",
    "### problem with hot encoding\n",
    "Problem:\n",
    "âœ” High-dimensional when dealing with thousands of categories.\n",
    "âœ” Sparse representation (mostly 0s), leading to memory inefficiency.\n",
    "âœ” No relationship captured between categories (New York and Los Angeles might be similar but appear as different as New York and Houston).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "493cc4f5-7af9-4196-8aa0-43ec34bb70c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 5, 4, 5, 10, 8]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cat_dims=[len(df[col].unique()) for col in ['MSSubClass','MSZoning','LotShape','LotConfig','OverallQual','OverallCond']]\n",
    "cat_dims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d113ac65-1d63-4ee9-96d5-7a729218d078",
   "metadata": {},
   "source": [
    "#### Thumb rules provide a quick and practical way to determine embedding dimensions without exhaustive hyperparameter tuning.\n",
    "\n",
    "EmbeddingÂ Dimension=min(50, root(NumberÂ ofÂ UniqueÂ Categories)\n",
    "â€‹\n",
    "ðŸ”¹ Explanation:\n",
    "\n",
    "Square root rule (âˆšn): Helps avoid overly large embeddings while still capturing useful patterns.\n",
    "Upper limit (50): Prevents unnecessarily large embeddings that may lead to overfitting.\n",
    "note:Overfitting Risk â€“ If embeddings are too large, they can memorize data instead of generalizing patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fb4b39a-571f-4a3e-a84e-d505b9bfee12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(15, 8), (5, 3), (4, 2), (5, 3), (10, 5), (8, 4)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# thumb rule output dimesion should be setbased on the input dimesion (min(50,feature_dim//2))\n",
    "embedding_dim=[(x,min(50,((x+1)//2))) for x in cat_dims]\n",
    "embedding_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47e70a5-7585-4992-9612-a5d8198763e6",
   "metadata": {},
   "source": [
    "#### ModuleList\n",
    "nn.ModuleList is a container for storing a list of nn.Module layers (like nn.Linear, nn.Conv2d, nn.Embedding, etc.). It registers each module as part of the model, ensuring that PyTorch tracks them properly for training, saving, and moving to devices (CPU/GPU).\n",
    "\n",
    "âœ… nn.ModuleList helps register layers in a model.\n",
    "âœ… It allows .to(device), .parameters(), and state_dict() to work properly.\n",
    "âœ… Useful for handling multiple embeddings, MLP layers, or recurrent layers dynamically.\n",
    "\n",
    "intead of python Module 'nn.module'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fb4d826-ac24-4dd2-9a0f-d23ed4475a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Embedding(15, 8)\n",
       "  (1): Embedding(5, 3)\n",
       "  (2): Embedding(4, 2)\n",
       "  (3): Embedding(5, 3)\n",
       "  (4): Embedding(10, 5)\n",
       "  (5): Embedding(8, 4)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn  # for Layer \n",
    "import torch.nn.functional as f  # for activation \n",
    "# Embedding  Layer\n",
    "embed_representation=nn.ModuleList([nn.Embedding(inp,out) for inp,out in embedding_dim])\n",
    "embed_representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed239af1-70f6-49e2-840d-1723f05f05d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 3, 3, 4, 6, 3],\n",
       "        [0, 3, 3, 2, 5, 6],\n",
       "        [5, 3, 0, 4, 6, 3],\n",
       "        ...,\n",
       "        [6, 3, 3, 4, 6, 7],\n",
       "        [0, 3, 3, 4, 4, 4],\n",
       "        [0, 3, 3, 4, 4, 4]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17a83e72-e2d7-4dff-9962-6dcc13bd79a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 3, 3, 4, 6, 3],\n",
       "        [0, 3, 3, 2, 5, 6],\n",
       "        [5, 3, 0, 4, 6, 3],\n",
       "        [6, 3, 0, 0, 6, 3]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for demo how to to work embeading \n",
    "#for  4 row\n",
    "cat_featz=cat_feature[:4]\n",
    "cat_featz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55758c45-5c5d-4645-9185-988fbad6bc17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.4885, -0.4020,  1.4327,  ..., -0.2818, -0.6909, -0.8620],\n",
       "         [ 0.2291, -0.0652, -2.4341,  ..., -0.5390, -0.3496, -0.4730],\n",
       "         [ 0.4885, -0.4020,  1.4327,  ..., -0.2818, -0.6909, -0.8620],\n",
       "         ...,\n",
       "         [-1.1192,  0.2129, -1.0304,  ...,  1.5235, -1.0064,  0.3443],\n",
       "         [ 0.2291, -0.0652, -2.4341,  ..., -0.5390, -0.3496, -0.4730],\n",
       "         [ 0.2291, -0.0652, -2.4341,  ..., -0.5390, -0.3496, -0.4730]],\n",
       "        grad_fn=<EmbeddingBackward0>),\n",
       " tensor([[-1.1752, -1.1975, -1.2106],\n",
       "         [-1.1752, -1.1975, -1.2106],\n",
       "         [-1.1752, -1.1975, -1.2106],\n",
       "         ...,\n",
       "         [-1.1752, -1.1975, -1.2106],\n",
       "         [-1.1752, -1.1975, -1.2106],\n",
       "         [-1.1752, -1.1975, -1.2106]], grad_fn=<EmbeddingBackward0>),\n",
       " tensor([[-0.0012,  0.6100],\n",
       "         [-0.0012,  0.6100],\n",
       "         [-0.0950, -0.4964],\n",
       "         ...,\n",
       "         [-0.0012,  0.6100],\n",
       "         [-0.0012,  0.6100],\n",
       "         [-0.0012,  0.6100]], grad_fn=<EmbeddingBackward0>),\n",
       " tensor([[-0.8926, -1.7854,  0.6679],\n",
       "         [-0.0173,  1.1753,  0.0134],\n",
       "         [-0.8926, -1.7854,  0.6679],\n",
       "         ...,\n",
       "         [-0.8926, -1.7854,  0.6679],\n",
       "         [-0.8926, -1.7854,  0.6679],\n",
       "         [-0.8926, -1.7854,  0.6679]], grad_fn=<EmbeddingBackward0>),\n",
       " tensor([[ 0.0733, -0.5459,  0.7621,  1.4915, -2.0235],\n",
       "         [-1.8824,  0.4478, -0.4650, -1.8775, -0.6603],\n",
       "         [ 0.0733, -0.5459,  0.7621,  1.4915, -2.0235],\n",
       "         ...,\n",
       "         [ 0.0733, -0.5459,  0.7621,  1.4915, -2.0235],\n",
       "         [ 0.0829,  1.5257, -0.8546,  0.0950,  0.5160],\n",
       "         [ 0.0829,  1.5257, -0.8546,  0.0950,  0.5160]],\n",
       "        grad_fn=<EmbeddingBackward0>),\n",
       " tensor([[ 5.0328e-01, -1.2872e+00, -9.4021e-01,  9.4488e-01],\n",
       "         [ 2.1257e-04, -6.7324e-01,  6.6461e-01,  1.1334e+00],\n",
       "         [ 5.0328e-01, -1.2872e+00, -9.4021e-01,  9.4488e-01],\n",
       "         ...,\n",
       "         [-3.4301e-01,  1.2878e+00, -2.4468e-01,  7.1584e-01],\n",
       "         [-3.3161e-01,  2.8656e+00, -5.3150e-01, -8.7184e-01],\n",
       "         [-3.3161e-01,  2.8656e+00, -5.3150e-01, -8.7184e-01]],\n",
       "        grad_fn=<EmbeddingBackward0>)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "embedding_val=[]\n",
    "for i,e in enumerate(embed_representation):\n",
    "    embedding_val.append(e(cat_feature[:,i]))\n",
    "embedding_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1cd6428c-ffaf-4fea-80e7-187bc2b82960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4885, -0.4020,  1.4327,  ..., -1.2872, -0.9402,  0.9449],\n",
       "        [ 0.2291, -0.0652, -2.4341,  ..., -0.6732,  0.6646,  1.1334],\n",
       "        [ 0.4885, -0.4020,  1.4327,  ..., -1.2872, -0.9402,  0.9449],\n",
       "        ...,\n",
       "        [-1.1192,  0.2129, -1.0304,  ...,  1.2878, -0.2447,  0.7158],\n",
       "        [ 0.2291, -0.0652, -2.4341,  ...,  2.8656, -0.5315, -0.8718],\n",
       "        [ 0.2291, -0.0652, -2.4341,  ...,  2.8656, -0.5315, -0.8718]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=torch.cat(embedding_val,1)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372092f6-524d-4d8b-870d-be221cf1d2be",
   "metadata": {},
   "source": [
    "stack vs cat\n",
    "stack is use with respect values of array but cat is use in embediing  \n",
    "which is itself of fucation of torch  but working is same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "568b2e57-0e46-42f8-81b5-03f1d59b0af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout=nn.Dropout(.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eca83c18-3aa4-47d2-9641-5e41db5be2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -0.0000,  2.3878,  ..., -2.1453, -1.5670,  0.0000],\n",
       "        [ 0.3818, -0.0000, -4.0568,  ..., -1.1221,  0.0000,  1.8890],\n",
       "        [ 0.8141, -0.0000,  0.0000,  ..., -0.0000, -1.5670,  1.5748],\n",
       "        ...,\n",
       "        [-1.8653,  0.0000, -0.0000,  ...,  0.0000, -0.0000,  0.0000],\n",
       "        [ 0.3818, -0.1086, -0.0000,  ...,  4.7761, -0.0000, -1.4531],\n",
       "        [ 0.3818, -0.1086, -4.0568,  ...,  4.7761, -0.0000, -1.4531]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_embed=dropout(z)      # lot of value is 0 because of dropout\n",
    "final_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a7ed64f-a869-4a67-9d1c-dc895f29ed0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000e+00, -0.0000e+00,  2.3878e+00, -3.7810e+00,  1.2468e+00,\n",
       "        -4.6965e-01, -0.0000e+00, -1.4366e+00, -0.0000e+00, -0.0000e+00,\n",
       "        -2.0176e+00, -2.0784e-03,  0.0000e+00, -0.0000e+00, -2.9757e+00,\n",
       "         1.1132e+00,  1.2213e-01, -9.0976e-01,  0.0000e+00,  2.4858e+00,\n",
       "        -3.3725e+00,  0.0000e+00, -2.1453e+00, -1.5670e+00,  0.0000e+00],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_embed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "626db770-df0b-4c45-b8de-5b838615390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a feed forward neural network\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self,embedding_dim,n_cont,layers,out_sz,p=0.5):\n",
    "        super().__init__()\n",
    "        self.embeds=nn.ModuleList(\n",
    "            [nn.Embedding(inp,out) for inp,out in embedding_dim]\n",
    "        )\n",
    "        self.emb_drop=nn.Dropout(p)\n",
    "        self.bn_cont=nn.BatchNorm1d(n_cont)\n",
    "        layerList=[]\n",
    "        n_emb=sum((out for inp,out in embedding_dim))\n",
    "        n_in=n_emb+n_cont\n",
    "\n",
    "        for i in layers:\n",
    "            layerList.append(nn.Linear(n_in,i))\n",
    "            layerList.append(nn.ReLU(inplace=True))\n",
    "            layerList.append(nn.BatchNorm1d(i))\n",
    "            layerList.append(nn.Dropout(p))\n",
    "            n_in=i\n",
    "        layerList.append(nn.Linear(layers[-1],out_sz))\n",
    "        self.layers=nn.Sequential(*layerList)\n",
    "        \n",
    "    def forward(self,x_cat,x_cont):\n",
    "        embedding=[]\n",
    "        for i,e in enumerate(self.embeds):\n",
    "            embedding.append(e(x_cat[:,i]))\n",
    "        x=torch.cat(embedding,axis=1)\n",
    "        x=self.emb_drop(x)\n",
    "\n",
    "        x_cont=self.bn_cont(x_cont)\n",
    "        x=torch.cat([x,x_cont],1)\n",
    "        x=self.layers(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db9f8d31-7663-49bf-b4ac-033d9933b986",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(100)\n",
    "model=FeedForwardNN(embedding_dim,len(continuous_feature),[100,50],1,p=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a257c11-5e51-4055-ac81-d2d3e0969139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForwardNN(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(15, 8)\n",
       "    (1): Embedding(5, 3)\n",
       "    (2): Embedding(4, 2)\n",
       "    (3): Embedding(5, 3)\n",
       "    (4): Embedding(10, 5)\n",
       "    (5): Embedding(8, 4)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.4, inplace=False)\n",
       "  (bn_cont): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=30, out_features=100, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.4, inplace=False)\n",
       "    (8): Linear(in_features=50, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "47243564-0831-47d8-93e2-80b38b87b4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_funcation=nn.MSELoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d3293c8-7c65-475f-a21f-914eebdef7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "batch_size=1200\n",
    "test_size=int(batch_size*0.15)  #15 %\n",
    "train_catg=cat_feature[:batch_size-test_size]\n",
    "test_catg=cat_feature[batch_size-test_size:batch_size]\n",
    "train_cont=continous_val[:batch_size-test_size]\n",
    "test_cont=continous_val[batch_size-test_size:batch_size]\n",
    "y_train=y[:batch_size-test_size]\n",
    "y_test=y[batch_size-test_size:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb64d7d0-8485-42b7-9214-474bc8936192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1020, 6]),\n",
       " torch.Size([180, 6]),\n",
       " torch.Size([1020, 5]),\n",
       " torch.Size([180, 5]),\n",
       " torch.Size([1020]),\n",
       " torch.Size([180]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_catg.shape,test_catg.shape,train_cont.shape,test_cont.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c3e6e39-add9-4d01-b8a9-c34962a441d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([1020])) that is different to the input size (torch.Size([1020, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number: 1 and the loss : 200496.625\n",
      "Epoch Number: 11 and the loss : 200496.609375\n",
      "Epoch Number: 21 and the loss : 200496.53125\n",
      "Epoch Number: 31 and the loss : 200496.484375\n",
      "Epoch Number: 41 and the loss : 200496.453125\n",
      "Epoch Number: 51 and the loss : 200496.390625\n",
      "Epoch Number: 61 and the loss : 200496.328125\n",
      "Epoch Number: 71 and the loss : 200496.25\n",
      "Epoch Number: 81 and the loss : 200496.15625\n",
      "Epoch Number: 91 and the loss : 200496.09375\n",
      "Epoch Number: 101 and the loss : 200495.953125\n",
      "Epoch Number: 111 and the loss : 200495.859375\n",
      "Epoch Number: 121 and the loss : 200495.6875\n",
      "Epoch Number: 131 and the loss : 200495.640625\n",
      "Epoch Number: 141 and the loss : 200495.4375\n",
      "Epoch Number: 151 and the loss : 200495.25\n",
      "Epoch Number: 161 and the loss : 200495.125\n",
      "Epoch Number: 171 and the loss : 200494.859375\n",
      "Epoch Number: 181 and the loss : 200494.65625\n",
      "Epoch Number: 191 and the loss : 200494.5\n",
      "Epoch Number: 201 and the loss : 200494.328125\n",
      "Epoch Number: 211 and the loss : 200494.015625\n",
      "Epoch Number: 221 and the loss : 200493.734375\n",
      "Epoch Number: 231 and the loss : 200493.484375\n",
      "Epoch Number: 241 and the loss : 200493.140625\n",
      "Epoch Number: 251 and the loss : 200492.90625\n",
      "Epoch Number: 261 and the loss : 200492.53125\n",
      "Epoch Number: 271 and the loss : 200492.21875\n",
      "Epoch Number: 281 and the loss : 200491.75\n",
      "Epoch Number: 291 and the loss : 200491.484375\n",
      "Epoch Number: 301 and the loss : 200491.171875\n",
      "Epoch Number: 311 and the loss : 200490.703125\n",
      "Epoch Number: 321 and the loss : 200490.140625\n",
      "Epoch Number: 331 and the loss : 200489.734375\n",
      "Epoch Number: 341 and the loss : 200489.34375\n",
      "Epoch Number: 351 and the loss : 200488.921875\n",
      "Epoch Number: 361 and the loss : 200488.515625\n",
      "Epoch Number: 371 and the loss : 200487.8125\n",
      "Epoch Number: 381 and the loss : 200487.390625\n",
      "Epoch Number: 391 and the loss : 200486.75\n",
      "Epoch Number: 401 and the loss : 200486.109375\n",
      "Epoch Number: 411 and the loss : 200485.53125\n",
      "Epoch Number: 421 and the loss : 200485.03125\n",
      "Epoch Number: 431 and the loss : 200484.359375\n",
      "Epoch Number: 441 and the loss : 200483.71875\n",
      "Epoch Number: 451 and the loss : 200482.828125\n",
      "Epoch Number: 461 and the loss : 200482.28125\n",
      "Epoch Number: 471 and the loss : 200481.609375\n",
      "Epoch Number: 481 and the loss : 200480.984375\n",
      "Epoch Number: 491 and the loss : 200480.1875\n",
      "Epoch Number: 501 and the loss : 200479.296875\n",
      "Epoch Number: 511 and the loss : 200478.828125\n",
      "Epoch Number: 521 and the loss : 200477.546875\n",
      "Epoch Number: 531 and the loss : 200476.765625\n",
      "Epoch Number: 541 and the loss : 200475.71875\n",
      "Epoch Number: 551 and the loss : 200474.953125\n",
      "Epoch Number: 561 and the loss : 200474.421875\n",
      "Epoch Number: 571 and the loss : 200473.28125\n",
      "Epoch Number: 581 and the loss : 200472.296875\n",
      "Epoch Number: 591 and the loss : 200471.296875\n",
      "Epoch Number: 601 and the loss : 200470.109375\n",
      "Epoch Number: 611 and the loss : 200469.125\n",
      "Epoch Number: 621 and the loss : 200468.21875\n",
      "Epoch Number: 631 and the loss : 200467.03125\n",
      "Epoch Number: 641 and the loss : 200466.0625\n",
      "Epoch Number: 651 and the loss : 200465.171875\n",
      "Epoch Number: 661 and the loss : 200463.578125\n",
      "Epoch Number: 671 and the loss : 200462.28125\n",
      "Epoch Number: 681 and the loss : 200461.046875\n",
      "Epoch Number: 691 and the loss : 200460.09375\n",
      "Epoch Number: 701 and the loss : 200458.890625\n",
      "Epoch Number: 711 and the loss : 200457.765625\n",
      "Epoch Number: 721 and the loss : 200456.421875\n",
      "Epoch Number: 731 and the loss : 200455.078125\n",
      "Epoch Number: 741 and the loss : 200453.375\n",
      "Epoch Number: 751 and the loss : 200452.359375\n",
      "Epoch Number: 761 and the loss : 200450.921875\n",
      "Epoch Number: 771 and the loss : 200449.125\n",
      "Epoch Number: 781 and the loss : 200447.859375\n",
      "Epoch Number: 791 and the loss : 200446.4375\n",
      "Epoch Number: 801 and the loss : 200444.78125\n",
      "Epoch Number: 811 and the loss : 200442.859375\n",
      "Epoch Number: 821 and the loss : 200441.828125\n",
      "Epoch Number: 831 and the loss : 200440.265625\n",
      "Epoch Number: 841 and the loss : 200438.5\n",
      "Epoch Number: 851 and the loss : 200436.875\n",
      "Epoch Number: 861 and the loss : 200435.375\n",
      "Epoch Number: 871 and the loss : 200433.296875\n",
      "Epoch Number: 881 and the loss : 200432.234375\n",
      "Epoch Number: 891 and the loss : 200429.8125\n",
      "Epoch Number: 901 and the loss : 200428.140625\n",
      "Epoch Number: 911 and the loss : 200426.90625\n",
      "Epoch Number: 921 and the loss : 200425.3125\n",
      "Epoch Number: 931 and the loss : 200423.03125\n",
      "Epoch Number: 941 and the loss : 200421.421875\n",
      "Epoch Number: 951 and the loss : 200419.1875\n",
      "Epoch Number: 961 and the loss : 200417.375\n",
      "Epoch Number: 971 and the loss : 200415.703125\n",
      "Epoch Number: 981 and the loss : 200413.421875\n",
      "Epoch Number: 991 and the loss : 200411.34375\n",
      "Epoch Number: 1001 and the loss : 200409.84375\n",
      "Epoch Number: 1011 and the loss : 200407.453125\n",
      "Epoch Number: 1021 and the loss : 200405.53125\n",
      "Epoch Number: 1031 and the loss : 200402.765625\n",
      "Epoch Number: 1041 and the loss : 200401.453125\n",
      "Epoch Number: 1051 and the loss : 200398.9375\n",
      "Epoch Number: 1061 and the loss : 200396.734375\n",
      "Epoch Number: 1071 and the loss : 200395.375\n",
      "Epoch Number: 1081 and the loss : 200392.65625\n",
      "Epoch Number: 1091 and the loss : 200390.09375\n",
      "Epoch Number: 1101 and the loss : 200387.75\n",
      "Epoch Number: 1111 and the loss : 200385.484375\n",
      "Epoch Number: 1121 and the loss : 200382.734375\n",
      "Epoch Number: 1131 and the loss : 200381.0\n",
      "Epoch Number: 1141 and the loss : 200378.859375\n",
      "Epoch Number: 1151 and the loss : 200375.859375\n",
      "Epoch Number: 1161 and the loss : 200373.53125\n",
      "Epoch Number: 1171 and the loss : 200372.34375\n",
      "Epoch Number: 1181 and the loss : 200368.828125\n",
      "Epoch Number: 1191 and the loss : 200366.109375\n",
      "Epoch Number: 1201 and the loss : 200363.171875\n",
      "Epoch Number: 1211 and the loss : 200361.859375\n",
      "Epoch Number: 1221 and the loss : 200359.328125\n",
      "Epoch Number: 1231 and the loss : 200355.984375\n",
      "Epoch Number: 1241 and the loss : 200354.109375\n",
      "Epoch Number: 1251 and the loss : 200351.53125\n",
      "Epoch Number: 1261 and the loss : 200347.1875\n",
      "Epoch Number: 1271 and the loss : 200344.890625\n",
      "Epoch Number: 1281 and the loss : 200342.625\n",
      "Epoch Number: 1291 and the loss : 200339.484375\n",
      "Epoch Number: 1301 and the loss : 200336.703125\n",
      "Epoch Number: 1311 and the loss : 200334.765625\n",
      "Epoch Number: 1321 and the loss : 200332.53125\n",
      "Epoch Number: 1331 and the loss : 200328.828125\n",
      "Epoch Number: 1341 and the loss : 200324.953125\n",
      "Epoch Number: 1351 and the loss : 200323.609375\n",
      "Epoch Number: 1361 and the loss : 200320.046875\n",
      "Epoch Number: 1371 and the loss : 200318.359375\n",
      "Epoch Number: 1381 and the loss : 200313.796875\n",
      "Epoch Number: 1391 and the loss : 200309.6875\n",
      "Epoch Number: 1401 and the loss : 200308.9375\n",
      "Epoch Number: 1411 and the loss : 200304.015625\n",
      "Epoch Number: 1421 and the loss : 200301.421875\n",
      "Epoch Number: 1431 and the loss : 200298.8125\n",
      "Epoch Number: 1441 and the loss : 200295.265625\n",
      "Epoch Number: 1451 and the loss : 200293.296875\n",
      "Epoch Number: 1461 and the loss : 200289.796875\n",
      "Epoch Number: 1471 and the loss : 200284.671875\n",
      "Epoch Number: 1481 and the loss : 200283.5\n",
      "Epoch Number: 1491 and the loss : 200280.375\n",
      "Epoch Number: 1501 and the loss : 200276.40625\n",
      "Epoch Number: 1511 and the loss : 200274.171875\n",
      "Epoch Number: 1521 and the loss : 200269.421875\n",
      "Epoch Number: 1531 and the loss : 200265.859375\n",
      "Epoch Number: 1541 and the loss : 200264.15625\n",
      "Epoch Number: 1551 and the loss : 200260.046875\n",
      "Epoch Number: 1561 and the loss : 200255.140625\n",
      "Epoch Number: 1571 and the loss : 200252.109375\n",
      "Epoch Number: 1581 and the loss : 200249.515625\n",
      "Epoch Number: 1591 and the loss : 200245.328125\n",
      "Epoch Number: 1601 and the loss : 200242.09375\n",
      "Epoch Number: 1611 and the loss : 200238.734375\n",
      "Epoch Number: 1621 and the loss : 200234.734375\n",
      "Epoch Number: 1631 and the loss : 200231.734375\n",
      "Epoch Number: 1641 and the loss : 200226.828125\n",
      "Epoch Number: 1651 and the loss : 200222.421875\n",
      "Epoch Number: 1661 and the loss : 200219.40625\n",
      "Epoch Number: 1671 and the loss : 200216.671875\n",
      "Epoch Number: 1681 and the loss : 200213.078125\n",
      "Epoch Number: 1691 and the loss : 200209.421875\n",
      "Epoch Number: 1701 and the loss : 200204.84375\n",
      "Epoch Number: 1711 and the loss : 200201.1875\n",
      "Epoch Number: 1721 and the loss : 200196.984375\n",
      "Epoch Number: 1731 and the loss : 200191.78125\n",
      "Epoch Number: 1741 and the loss : 200191.3125\n",
      "Epoch Number: 1751 and the loss : 200185.125\n",
      "Epoch Number: 1761 and the loss : 200181.578125\n",
      "Epoch Number: 1771 and the loss : 200178.9375\n",
      "Epoch Number: 1781 and the loss : 200173.40625\n",
      "Epoch Number: 1791 and the loss : 200168.3125\n",
      "Epoch Number: 1801 and the loss : 200165.140625\n",
      "Epoch Number: 1811 and the loss : 200163.5\n",
      "Epoch Number: 1821 and the loss : 200157.140625\n",
      "Epoch Number: 1831 and the loss : 200153.734375\n",
      "Epoch Number: 1841 and the loss : 200147.421875\n",
      "Epoch Number: 1851 and the loss : 200145.671875\n",
      "Epoch Number: 1861 and the loss : 200143.359375\n",
      "Epoch Number: 1871 and the loss : 200137.046875\n",
      "Epoch Number: 1881 and the loss : 200131.890625\n",
      "Epoch Number: 1891 and the loss : 200128.75\n",
      "Epoch Number: 1901 and the loss : 200122.359375\n",
      "Epoch Number: 1911 and the loss : 200120.359375\n",
      "Epoch Number: 1921 and the loss : 200113.625\n",
      "Epoch Number: 1931 and the loss : 200109.703125\n",
      "Epoch Number: 1941 and the loss : 200107.09375\n",
      "Epoch Number: 1951 and the loss : 200103.421875\n",
      "Epoch Number: 1961 and the loss : 200098.78125\n",
      "Epoch Number: 1971 and the loss : 200094.734375\n",
      "Epoch Number: 1981 and the loss : 200088.03125\n",
      "Epoch Number: 1991 and the loss : 200081.0625\n",
      "Epoch Number: 2001 and the loss : 200078.078125\n",
      "Epoch Number: 2011 and the loss : 200075.703125\n",
      "Epoch Number: 2021 and the loss : 200068.71875\n",
      "Epoch Number: 2031 and the loss : 200065.984375\n",
      "Epoch Number: 2041 and the loss : 200058.953125\n",
      "Epoch Number: 2051 and the loss : 200056.828125\n",
      "Epoch Number: 2061 and the loss : 200052.28125\n",
      "Epoch Number: 2071 and the loss : 200048.796875\n",
      "Epoch Number: 2081 and the loss : 200043.40625\n",
      "Epoch Number: 2091 and the loss : 200036.296875\n",
      "Epoch Number: 2101 and the loss : 200030.796875\n",
      "Epoch Number: 2111 and the loss : 200028.578125\n",
      "Epoch Number: 2121 and the loss : 200024.015625\n",
      "Epoch Number: 2131 and the loss : 200018.75\n",
      "Epoch Number: 2141 and the loss : 200011.8125\n",
      "Epoch Number: 2151 and the loss : 200007.25\n",
      "Epoch Number: 2161 and the loss : 200000.484375\n",
      "Epoch Number: 2171 and the loss : 200000.734375\n",
      "Epoch Number: 2181 and the loss : 199993.828125\n",
      "Epoch Number: 2191 and the loss : 199985.109375\n",
      "Epoch Number: 2201 and the loss : 199983.46875\n",
      "Epoch Number: 2211 and the loss : 199976.765625\n",
      "Epoch Number: 2221 and the loss : 199973.921875\n",
      "Epoch Number: 2231 and the loss : 199969.984375\n",
      "Epoch Number: 2241 and the loss : 199964.265625\n",
      "Epoch Number: 2251 and the loss : 199953.84375\n",
      "Epoch Number: 2261 and the loss : 199954.046875\n",
      "Epoch Number: 2271 and the loss : 199948.9375\n",
      "Epoch Number: 2281 and the loss : 199944.140625\n",
      "Epoch Number: 2291 and the loss : 199939.234375\n",
      "Epoch Number: 2301 and the loss : 199931.0625\n",
      "Epoch Number: 2311 and the loss : 199924.453125\n",
      "Epoch Number: 2321 and the loss : 199921.125\n",
      "Epoch Number: 2331 and the loss : 199913.15625\n",
      "Epoch Number: 2341 and the loss : 199911.75\n",
      "Epoch Number: 2351 and the loss : 199906.59375\n",
      "Epoch Number: 2361 and the loss : 199900.09375\n",
      "Epoch Number: 2371 and the loss : 199887.359375\n",
      "Epoch Number: 2381 and the loss : 199883.9375\n",
      "Epoch Number: 2391 and the loss : 199883.71875\n",
      "Epoch Number: 2401 and the loss : 199876.703125\n",
      "Epoch Number: 2411 and the loss : 199871.765625\n",
      "Epoch Number: 2421 and the loss : 199866.90625\n",
      "Epoch Number: 2431 and the loss : 199853.75\n",
      "Epoch Number: 2441 and the loss : 199851.421875\n",
      "Epoch Number: 2451 and the loss : 199844.890625\n",
      "Epoch Number: 2461 and the loss : 199843.734375\n",
      "Epoch Number: 2471 and the loss : 199834.75\n",
      "Epoch Number: 2481 and the loss : 199831.40625\n",
      "Epoch Number: 2491 and the loss : 199826.59375\n",
      "Epoch Number: 2501 and the loss : 199822.53125\n",
      "Epoch Number: 2511 and the loss : 199809.875\n",
      "Epoch Number: 2521 and the loss : 199809.59375\n",
      "Epoch Number: 2531 and the loss : 199798.875\n",
      "Epoch Number: 2541 and the loss : 199791.34375\n",
      "Epoch Number: 2551 and the loss : 199796.25\n",
      "Epoch Number: 2561 and the loss : 199780.6875\n",
      "Epoch Number: 2571 and the loss : 199781.53125\n",
      "Epoch Number: 2581 and the loss : 199764.359375\n",
      "Epoch Number: 2591 and the loss : 199766.109375\n",
      "Epoch Number: 2601 and the loss : 199757.828125\n",
      "Epoch Number: 2611 and the loss : 199756.484375\n",
      "Epoch Number: 2621 and the loss : 199741.140625\n",
      "Epoch Number: 2631 and the loss : 199743.03125\n",
      "Epoch Number: 2641 and the loss : 199733.453125\n",
      "Epoch Number: 2651 and the loss : 199725.796875\n",
      "Epoch Number: 2661 and the loss : 199727.421875\n",
      "Epoch Number: 2671 and the loss : 199714.328125\n",
      "Epoch Number: 2681 and the loss : 199712.171875\n",
      "Epoch Number: 2691 and the loss : 199702.453125\n",
      "Epoch Number: 2701 and the loss : 199698.734375\n",
      "Epoch Number: 2711 and the loss : 199692.015625\n",
      "Epoch Number: 2721 and the loss : 199684.5625\n",
      "Epoch Number: 2731 and the loss : 199677.734375\n",
      "Epoch Number: 2741 and the loss : 199672.640625\n",
      "Epoch Number: 2751 and the loss : 199671.953125\n",
      "Epoch Number: 2761 and the loss : 199662.703125\n",
      "Epoch Number: 2771 and the loss : 199649.953125\n",
      "Epoch Number: 2781 and the loss : 199651.4375\n",
      "Epoch Number: 2791 and the loss : 199639.578125\n",
      "Epoch Number: 2801 and the loss : 199634.0\n",
      "Epoch Number: 2811 and the loss : 199630.578125\n",
      "Epoch Number: 2821 and the loss : 199623.8125\n",
      "Epoch Number: 2831 and the loss : 199614.09375\n",
      "Epoch Number: 2841 and the loss : 199605.59375\n",
      "Epoch Number: 2851 and the loss : 199599.828125\n",
      "Epoch Number: 2861 and the loss : 199595.90625\n",
      "Epoch Number: 2871 and the loss : 199585.4375\n",
      "Epoch Number: 2881 and the loss : 199578.4375\n",
      "Epoch Number: 2891 and the loss : 199572.796875\n",
      "Epoch Number: 2901 and the loss : 199562.96875\n",
      "Epoch Number: 2911 and the loss : 199557.453125\n",
      "Epoch Number: 2921 and the loss : 199560.390625\n",
      "Epoch Number: 2931 and the loss : 199540.1875\n",
      "Epoch Number: 2941 and the loss : 199533.375\n",
      "Epoch Number: 2951 and the loss : 199529.875\n",
      "Epoch Number: 2961 and the loss : 199525.90625\n",
      "Epoch Number: 2971 and the loss : 199514.3125\n",
      "Epoch Number: 2981 and the loss : 199503.765625\n",
      "Epoch Number: 2991 and the loss : 199502.21875\n"
     ]
    }
   ],
   "source": [
    "epochs=3000\n",
    "final_losses=[]\n",
    "for i in range(epochs):\n",
    "    i=i+1\n",
    "    y_pred=model(train_catg,train_cont)\n",
    "    loss=torch.sqrt(loss_funcation(y_pred,y_train))\n",
    "    final_losses.append(loss)\n",
    "    if i%10==1:\n",
    "        print(\"Epoch Number: {} and the loss : {}\".format(i,loss.item()))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0bfcfa11-5b98-4d25-8c4f-9d80c1a71baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Training Loss Curve')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAHFCAYAAAAwv7dvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABeCUlEQVR4nO3deVhUZf8G8HvYhn0AWcYRFMw9XBL3MtzABbd6S0tTybLX3DK33FJfMzEz61Uz216tLMkyy3JFEcwERQUF9wUUFURlU3aY5/eHP08dBxR04Axwf65rrpzveeac75wm5+6cM89RCSEEiIiIiOixmSndABEREVFNwWBFREREZCQMVkRERERGwmBFREREZCQMVkRERERGwmBFREREZCQMVkRERERGwmBFREREZCQMVkRERERGwmBFRAAAlUpVrkdERMRjbWfBggVQqVSP9NqIiAij9PA42/7555+rfNuP4vjx43j11Vfh4+MDa2tr2Nvbo23btli6dCnS09OVbo+oxrJQugEiMg1RUVGy5++99x727t2L8PBwWb1FixaPtZ3XX38dffr0eaTXtm3bFlFRUY/dQ0335ZdfYty4cWjatCmmT5+OFi1aoKioCIcPH8aaNWsQFRWFzZs3K90mUY3EYEVEAIBOnTrJnru5ucHMzMygfr/c3FzY2tqWezuenp7w9PR8pB4dHR0f2k9tFxUVhTfffBMBAQH49ddfoVarpWUBAQGYOnUqduzYYZRt5eXlwdra+pGPQBLVRDwVSETl1q1bN/j6+mLfvn3o0qULbG1tMXr0aADAjz/+iMDAQNStWxc2NjZo3rw5Zs6ciZycHNk6SjsV6O3tjf79+2PHjh1o27YtbGxs0KxZM/zvf/+TjSvtVGBwcDDs7e1x/vx59OvXD/b29vDy8sLUqVNRUFAge/2VK1fwwgsvwMHBAU5OThg+fDhiYmKgUqmwbt06o+yjhIQEDBo0CM7OzrC2tkabNm3wzTffyMbo9XosWrQITZs2hY2NDZycnNCqVSv897//lcbcuHEDb7zxBry8vKBWq+Hm5oann34au3fvfuD2Fy9eDJVKhS+++EIWqu6xsrLCwIEDpecqlQoLFiwwGOft7Y3g4GDp+bp166BSqbBr1y6MHj0abm5usLW1xY8//giVSoU9e/YYrOOzzz6DSqXC8ePHpdrhw4cxcOBAuLi4wNraGk899RQ2btz4wPdEVJ3wiBURVUhKSgpeeeUVzJgxA4sXL4aZ2d3/Pzt37hz69euHyZMnw87ODqdPn8YHH3yAQ4cOGZxOLM2xY8cwdepUzJw5Ex4eHvjqq6/w2muvoVGjRnj22Wcf+NqioiIMHDgQr732GqZOnYp9+/bhvffeg0ajwbx58wAAOTk56N69O9LT0/HBBx+gUaNG2LFjB4YOHfr4O+X/nTlzBl26dIG7uztWrFiBOnXqYP369QgODsb169cxY8YMAMDSpUuxYMECzJ07F88++yyKiopw+vRpZGZmSusaMWIEjh49ivfffx9NmjRBZmYmjh49ilu3bpW5/ZKSEoSHh8PPzw9eXl5Ge1//NHr0aAQFBeG7775DTk4O+vfvD3d3d6xduxY9e/aUjV23bh3atm2LVq1aAQD27t2LPn36oGPHjlizZg00Gg1CQ0MxdOhQ5ObmyoIcUbUliIhKMWrUKGFnZyer+fv7CwBiz549D3ytXq8XRUVFIjIyUgAQx44dk5bNnz9f3P9XT4MGDYS1tbW4dOmSVMvLyxMuLi7i3//+t1Tbu3evACD27t0r6xOA2Lhxo2yd/fr1E02bNpWef/rppwKA2L59u2zcv//9bwFArF279oHv6d62f/rppzLHvPTSS0KtVovLly/L6n379hW2trYiMzNTCCFE//79RZs2bR64PXt7ezF58uQHjrlfamqqACBeeumlcr8GgJg/f75BvUGDBmLUqFHS87Vr1woAYuTIkQZjp0yZImxsbKT3J4QQJ0+eFADEypUrpVqzZs3EU089JYqKimSv79+/v6hbt64oKSkpd99EpoqnAomoQpydndGjRw+D+sWLFzFs2DBotVqYm5vD0tIS/v7+AIBTp049dL1t2rRB/fr1pefW1tZo0qQJLl269NDXqlQqDBgwQFZr1aqV7LWRkZFwcHAwuHD+5Zdffuj6yys8PBw9e/Y0OFoUHByM3Nxc6QcCHTp0wLFjxzBu3Djs3LkT2dnZBuvq0KED1q1bh0WLFiE6OhpFRUVG6/Nx/Otf/zKojR49Gnl5efjxxx+l2tq1a6FWqzFs2DAAwPnz53H69GkMHz4cAFBcXCw9+vXrh5SUFJw5c6Zq3gRRJWKwIqIKqVu3rkHtzp076Nq1Kw4ePIhFixYhIiICMTEx+OWXXwDcvcj5YerUqWNQU6vV5Xqtra0trK2tDV6bn58vPb916xY8PDwMXlta7VHdunWr1P2j0+mk5QAwa9YsLFu2DNHR0ejbty/q1KmDnj174vDhw9JrfvzxR4waNQpfffUVOnfuDBcXF4wcORKpqallbt/V1RW2trZITEw02nu6X2nv78knn0T79u2xdu1aAHdPSa5fvx6DBg2Ci4sLAOD69esAgGnTpsHS0lL2GDduHADg5s2bldY3UVXhNVZEVCGl/QIsPDwc165dQ0REhHSUCoDsmiGl1alTB4cOHTKoPyioPMo2UlJSDOrXrl0DcDf4AICFhQWmTJmCKVOmIDMzE7t378bs2bPRu3dvJCcnw9bWFq6urvjkk0/wySef4PLly9iyZQtmzpyJtLS0Mn/VZ25ujp49e2L79u24cuVKuX59qVarDS7yB1DmtVxl/QLw1Vdfxbhx43Dq1ClcvHgRKSkpePXVV6Xl9977rFmz8Pzzz5e6jqZNmz60XyJTxyNWRPTY7n3Z3v8rtM8//1yJdkrl7++P27dvY/v27bJ6aGio0bbRs2dPKWT+07fffgtbW9tSp4pwcnLCCy+8gPHjxyM9PR1JSUkGY+rXr48JEyYgICAAR48efWAPs2bNghACY8aMQWFhocHyoqIi/P7779Jzb29v2a/2gLtB+c6dOw/czv1efvllWFtbY926dVi3bh3q1auHwMBAaXnTpk3RuHFjHDt2DO3atSv14eDgUKFtEpkiHrEiosfWpUsXODs7Y+zYsZg/fz4sLS3x/fff49ixY0q3Jhk1ahQ+/vhjvPLKK1i0aBEaNWqE7du3Y+fOnQAg/brxYaKjo0ut+/v7Y/78+fjjjz/QvXt3zJs3Dy4uLvj++++xdetWLF26FBqNBgAwYMAA+Pr6ol27dnBzc8OlS5fwySefoEGDBmjcuDGysrLQvXt3DBs2DM2aNYODgwNiYmKwY8eOMo/23NO5c2d89tlnGDduHPz8/PDmm2/iySefRFFREWJjY/HFF1/A19dXuiZtxIgRePfddzFv3jz4+/vj5MmTWLVqldRreTk5OeG5557DunXrkJmZiWnTphns088//xx9+/ZF7969ERwcjHr16iE9PR2nTp3C0aNH8dNPP1Vom0SmiMGKiB5bnTp1sHXrVkydOhWvvPIK7OzsMGjQIPz4449o27at0u0BAOzs7BAeHo7JkydjxowZUKlUCAwMxOrVq9GvXz84OTmVaz0fffRRqfW9e/eiW7duOHDgAGbPno3x48cjLy8PzZs3x9q1a2VTCXTv3h2bNm3CV199hezsbGi1WgQEBODdd9+FpaUlrK2t0bFjR3z33XdISkpCUVER6tevj3feeUeasuFBxowZgw4dOuDjjz/GBx98gNTUVFhaWqJJkyYYNmwYJkyYII2dPn06srOzsW7dOixbtgwdOnTAxo0bMWjQoHLtj3969dVXsWHDBgAodeqE7t2749ChQ3j//fcxefJkZGRkoE6dOmjRogWGDBlS4e0RmSKVEEIo3QQRkVIWL16MuXPn4vLly488IzwR0T08YkVEtcaqVasAAM2aNUNRURHCw8OxYsUKvPLKKwxVRGQUDFZEVGvY2tri448/RlJSEgoKCqTTa3PnzlW6NSKqIXgqkIiIiMhION0CERERkZEwWBEREREZCYMVERERkZHw4vUqptfrce3aNTg4OJR5awgiIiIyLUII3L59Gzqd7oETCjNYVbFr164Z3PmeiIiIqofk5OQHTs/CYFXF7t0LKzk5GY6Ojgp3Q0REROWRnZ0NLy+vh97TksGqit07/efo6MhgRUREVM087DIeXrxOREREZCQMVkRERERGwmBFREREZCQMVkRERERGwmBFREREZCQMVkRERERGwmBFREREZCQMVkRERERGwmBFREREZCQMVkRERERGwmBFREREZCQMVkRERERGwpsw1xBXM/OgAmBupoKZSgUz1f//2UwFc5UK5mYqqFSApZkZzMwefANJIiIiejQMVjVE92URKCzWP3ScSgVYmZvB0twMFuYqWJqbweoff7a1Moe1pTlsrczv+7NFqXUby7vLrCzMoLGxgNrCHI7WltDYWlbBuyYiIjItDFY1hNrcDBBAiRAo0YsyxwkBFBTrUVCOEPa43B3UsLe2QF2NNVzt1XCxs5L+WcfOCnXs1ahjZwVLCzPUdbTmkTQiIqr2VEKIsr+Fyeiys7Oh0WiQlZUFR0fHStuO+P+AVSIE9Pq7gUsvBPKLSlBYrEeJXqCoRI/C4rv/vPsQyCsqRm5hCfIKS5BXdPefuQZ/Lpb+nF9UgjsFxUi7XQAIoEivR37Ro4U2dwc1PByt4WpvBXtrS+icrGFraYEmHvZo6GaP+i62sLIwgzkDGBERVbHyfn/ziFUNpVKpYGGuMvgX7Ghd+afosvOLkJFTiNv5xcjOL0JqVj7Scwpx804h0nMKcOtOIW7m3P1z+p1C5BSWAADSbhfcDWgPYGtlDjcHNeq72OIJN3t4udiinpMNGrnbw8fVjqGLiIgUxWBFRudobVmhAFdQXIKMnCJcy8pDRk4hLt7IweX0XJxPu4OC4hLcvFOI1Ox8FBbrkVtYgku3cnHpVi7+PHez1PV517FFK08ndG3sCm9XO/i42qGOnRVUKoYuIiKqXDwVWMWq6lRgTVOiF8jKK8L5tDs4k5qNo5czkV9UglMp2biUnovyfIqfcLNDK08neDrboKNPHTTVOsDZ1hIW5px1hIiIHqy8398MVlWMwcr4hLgbuk6l3Mbl9BxEXbiFWzmFyMgtxLnrdx56oX5rTw2equ+Mfi3roqnWARob/qKRiIjkGKxMFINV1UtOz0Vscib2nLqOs9fv4MbtfNy8U1jmeBtLc+QV3b3ua9FgXwxqo4NDFVybRkREpovBykQxWJmG4hI9TqZk4/N9FwEAF9Lu4MKNOygqKfs/B09nGzzTyBWvd/VBI3eHqmqViIhMAIOViWKwMl1CCJxPu4OYpAws2noSuf//a8XSqFSAg9oCfX3rYkh7TzSv6whbK/4WhIiopmKwMlEMVtVLVl4RIs6k4VBiOr4/ePmh41t7OWFSj0bo1tSdUz8QEdUgDFYmisGq+jufdgd/HL+GtX8loUQvcKeguNRxrvZW6OtbF8M61kfzuvx3TURUnTFYmSgGq5onPacQvxy9gi/2XXzoBKf/auuJRYN9obbgzbCJiKoTBisTxWBVs+n1Atey8rDx8BWs2HPugWO7NnbFm/5PoLWXE+zUvD6LiMiUMViZKAar2kUIgUOJ6VgedhYHE9PLHNfQzQ79/v+0oc7Jpgo7JCKi8mCwMlEMVrWXEAIpWfnYeyYN30dfxsmU7DLHTu/dFCM7N+D8WUREJoLBykQxWNE/xSVn4psDSfjr/M0yr88a1rE+xndvhHo8kkVEpBgGKxPFYEVl+ev8Tbz9Y1yZActBbYEPX2yNXs3deX9DIqIqxmBlohisqDxiL2dgc+xVfBt1qcwxrz7tjUk9GsPZzqoKOyMiqp0YrEwUgxVVVHZ+Ed5cfwSHEtNLveVOK08N/Bo4450+zWBtaa5Ah0RENR+DlYlisKLHce76bbz0RTRu5ZR+E2nfeo748IXWaKZ1gErFebKIiIyFwcpEMViRsVy4cQf/259Y5q12pgY0wWtdfXgPQyIiI2CwMlEMVlQZ3t96El/+mVjm8iXPt8TQ9l48ikVE9IgYrEwUgxVVpqy8Ikz5MQ57TqeVuvzlDl74V1tP+DVwZsgiIqoABisTxWBFVeV82h2sj76EdQeSSl0e8nxLvNyhftU2RURUTTFYmSgGK6pqOQXFCNl+CuujS78Wq/eTHlj2YmvO8k5E9AAMViaKwYqUotcLnE27jdFrY3AtK99geY9m7pjZtxkaudnDzIynCYmI/onBykQxWJEpuJ6dj9+PXcOiradKXd77SQ+sfLktrCw4wzsREcBgZbIYrMjU7Dl1Ha99c7jUZQEtPLB6eFtY8hY6RFTLlff7W9G/LUNCQtC+fXs4ODjA3d0dgwcPxpkzZ2RjhBBYsGABdDodbGxs0K1bN5w4cUI2pqCgABMnToSrqyvs7OwwcOBAXLlypdRtFhQUoE2bNlCpVIiLi5Mtu3z5MgYMGAA7Ozu4urpi0qRJKCyUT8QYHx8Pf39/2NjYoF69eli4cCGYTak669ncA0lLgrB10jOo72IrWxZ28joaz9kO75lbcT7ttkIdEhFVH4oGq8jISIwfPx7R0dEICwtDcXExAgMDkZOTI41ZunQpli9fjlWrViEmJgZarRYBAQG4ffvvv+QnT56MzZs3IzQ0FPv378edO3fQv39/lJSUGGxzxowZ0Ol0BvWSkhIEBQUhJycH+/fvR2hoKDZt2oSpU6dKY7KzsxEQEACdToeYmBisXLkSy5Ytw/Lly428Z4iq3pM6DfbN6I7T7/VBa0+NwfJey/dh/A9HkZyeq0B3RETVhDAhaWlpAoCIjIwUQgih1+uFVqsVS5Yskcbk5+cLjUYj1qxZI4QQIjMzU1haWorQ0FBpzNWrV4WZmZnYsWOHbP3btm0TzZo1EydOnBAARGxsrGyZmZmZuHr1qlTbsGGDUKvVIisrSwghxOrVq4VGoxH5+fnSmJCQEKHT6YRery/Xe8zKyhIApHUSmaqUzDwReuiSaPDOH6U+fj92VZSUlO9zT0RU3ZX3+9ukLpzIysoCALi4uAAAEhMTkZqaisDAQGmMWq2Gv78/Dhw4AAA4cuQIioqKZGN0Oh18fX2lMQBw/fp1jBkzBt999x1sbeWnOwAgKioKvr6+sqNZvXv3RkFBAY4cOSKN8ff3h1qtlo25du0akpKSSn1PBQUFyM7Olj2IqgOtxhpD29dH0pIgLP1XK4PlE36IRcPZ2xB28roC3RERmSaTCVZCCEyZMgXPPPMMfH19AQCpqakAAA8PD9lYDw8PaVlqaiqsrKzg7Oxc5hghBIKDgzF27Fi0a9eu1O2npqYabMfZ2RlWVlaybZXWyz97vV9ISAg0Go308PLyevCOIDJBQ9p7IWlJEELf6ITG7vayZWO+PYyGs7Yi9nIGMsq4OTQRUW1hMsFqwoQJOH78ODZs2GCw7P5bbwghHno7jn+OWblyJbKzszFr1qwHvqa0dd6/rdJ6Keu1ADBr1ixkZWVJj+Tk5Af2QGTKOjWsg7Ap/jg8txf6tdRKdb0Anlt9AE+9F4YRXx9EcYlewS6JiJRjEsFq4sSJ2LJlC/bu3QtPT0+prtXe/Yv7/qNBaWlp0pEirVaLwsJCZGRklDkmPDwc0dHRUKvVsLCwQKNGjQAA7dq1w6hRo6T13L+djIwMFBUVybZVWi+A4VG1e9RqNRwdHWUPourO1V6N1cP9sG96d4Nlf567iUZztmPLsWsKdEZEpCxFg5UQAhMmTMAvv/yC8PBw+Pj4yJb7+PhAq9UiLCxMqhUWFiIyMhJdunQBAPj5+cHS0lI2JiUlBQkJCdKYFStW4NixY4iLi0NcXBy2bdsGAPjxxx/x/vvvAwA6d+6MhIQEpKSkSOvZtWsX1Go1/Pz8pDH79u2TTcGwa9cu6HQ6eHt7G3HPEFUP9evYImlJECKnd8OA1vJf207aEAvvmVuRcDVLoe6IiKqeohOEjhs3Dj/88AN+++03NG3aVKprNBrY2NgAAD744AOEhIRg7dq1aNy4MRYvXoyIiAicOXMGDg4OAIA333wTf/zxB9atWwcXFxdMmzYNt27dwpEjR2Bubm6w3aSkJPj4+CA2NhZt2rQBcHe6hTZt2sDDwwMffvgh0tPTERwcjMGDB2PlypUA7l5c37RpU/To0QOzZ8/GuXPnEBwcjHnz5smmZXgQThBKNVleYQkGfbofZ6/fkdWdbC3x5ch2aO/tolBnRESPp1rMvF7WdUlr165FcHAwgLtHtf7zn//g888/R0ZGBjp27IhPP/1UusAdAPLz8zF9+nT88MMPyMvLQ8+ePbF69eoyLxQvLVgBdycIHTduHMLDw2FjY4Nhw4Zh2bJlsl8BxsfHY/z48Th06BCcnZ0xduxYzJs376HXfN3DYEW1wfErmRj5v0PIzC0yWPbT2M4MWERU7VSLYFUbMVhRbXI7vwhvfHsEURdvGSwLn+qPhm72pbyKiMj0MFiZKAYrqo3yCkuwMvwcVkdcMFi2/53u8HQ2nFuOiMiUMFiZKAYrqs0u3cqB/4cRpS5b8fJTGNja8HZTRESmoFrchJmIapcGdeyQtCQIu6c8a7Bs0oZYrIm8gPwiw3t8EhFVFzxiVcV4xIroLiEE1h+8jHd/TTBY5lvPEX9M7KpAV0REpeMRKyIyaSqVCiM6NUBiSD/M7NtMtizhaja8Z25F6KHL4P/7EVF1wiNWVYxHrIhKl1tYjIk/xGLP6TSDZfumd4eXi025pzUhIjI2HrEiomrF1soCXwe3x/rXOhose/bDvQhasR/Z+YbzYhERmRIesapiPGJFVD5RF27h5S+jDeovtffCnKDmcLC2VKArIqqteMSKiKq1zk/UwcmFveHmoJbVQ2OS0XLBLqwKP6dQZ0REZeMRqyrGI1ZEFVdUokfjOdtLXXZkbi/UsVeXuoyIyFh4xIqIagxLczMkLQlC5PRuBsv8Fu3G2r8S+etBIjIJPGJVxXjEiujxJd3MQbdlEQb155+qh3HdG6GRO+9BSETGxSNWRFRjebva4cR/eqO1l5Os/kvsVfRaHsmjV0SkGAYrIqqW7NQW+G3806WeHvSZtQ0xSelV3xQR1XoMVkRUrd27/+Ciwb6y+otrotDh/d24dCsHJXoewSKiqsFrrKoYr7Eiqjy5hcV44bMonEzJltUdrS0QOb07nO2sFOqMiKo7XmNFRLWOrZUFtr3VFX9MfEZWz84vxlPvhSG/qEShzoiotmCwIqIax7eeBmcX9UX3pm6yerN3d2BN5AWkZecr1BkR1XQ8FVjFeCqQqGqdT7uDl76Iws07hbL6sfmB0NjwtjhEVD48FUhEBKCRuz32v9MD03s3ldVb/2cXXv/mMP48d0OhzoioJmKwIqIaz9rSHOO7N8Kh2T3Rsp5Gqu8+dR0jvj6EgxdvKdgdEdUkDFZEVGu4O1pjy4Sn8a+2nrL60C+i8dq6GF7cTkSPjddYVTFeY0VkGpJu5mDgqv3Izi+W1YO7eOOdPs1gY2WuUGdEZIp4jRUR0QN4u9rh+ILe2P5WV1l93YEkjF4Xg6ISvUKdEVF1xmBFRLVa87qOSFoShFaef197FXXxFgau+guFxQxXRFQxDFZERADWv94Rm97sAld7NQDgVEo2mszdjgVbTiA7v0jh7oioumCwIiIC4GhtCb8GzoiZ0xNW5n//1bjuQBJaLdiFtNucVJSIHo7BiojoH1QqFfa/0x0WZipZvcP7ezD313gkXM1SqDMiqg74q8Aqxl8FElUfRy6l41+fRRnU4xcEwsGas7YT1Sb8VSAR0WPya+CCpCVBeLd/C1m95YJdWLrjNEr0/P9SIpJjsCIieojXnvHBlglPy2qrIy7gidnbFOqIiEwVgxURUTm08nTCnH7NDereM7ci/PR1BToiIlPEa6yqGK+xIqr+vj94CXM2J8hqWkdrHJjZA2b3XfRORDUDr7EiIqokwzs2wFcj28lqqdn5aDh7G1Ky8hTqiohMAYMVEdEj6NXCA+ff74vhHevL6p1DwvHimgPgyQCi2onBiojoEVmYm+H951pi87gusnpMUgZazNuJvMIShTojIqUwWBERPaan6jsjbl6ArJZXVILm83Yg4kyaQl0RkRIYrIiIjMDJ1gpJS4IwtJ2XrB68NgYbDl3Gbd5vkKhW4K8Cqxh/FUhU851Pu4NeyyMN6gdm9oDOyUaBjojocfFXgURECmnkbo+/ZvYwqHdZEo5sHrkiqtEYrIiIKkE9Jxv8ct9F7QDQasEuZOQUKtAREVUFBisiokrStr4zEkP6YWBrnaz+1HthaP/+boW6IqLKxGBFRFSJVCoVVrz8FM6/31dWv3G7AN4zt+LSrRyFOiOiysBgRURUBSzMzRA9q6dB3f/DCCTdZLgiqikYrIiIqohWY42Li/vh46GtZfVuyyKwbOcZhboiImNisCIiqkJmZio895Qnfnyjk6y+au95TP/pGG+FQ1TNMVgRESmgY8M6SAzpJ6v9dOQKfGZtQ1GJXqGuiOhxMVgRESlEpVIhZk4vdHmijqzeeM52JPK6K6JqicGKiEhBbg5qrH+tI/q3qiurd18WgdfWxfDUIFE1w2BFRKQwMzMVVg1ri4Oz5b8a3HM6DUM/j1aoKyJ6FAxWREQmwsPR2uBWOIeS0uE9cysyczlbO1F1wGBFRGRC6jnZ4Oyivmjq4SCrt1kYhqOXM1Ci56lBIlPGYEVEZGKsLMyw/a2ucHdQy+rPrz6ADznfFZFJY7AiIjJBZmYqHJzdE8+3rSerr4m8gM2xVxTqiogeRiX4k5MqlZ2dDY1Gg6ysLDg6OirdDhFVA5du5aD/iv24XVAsq0dM6wZvVzuFuiKqXcr7/c0jVkREJq5BHTscmx9oUH9hzQGk5/CidiJTomiwCgkJQfv27eHg4AB3d3cMHjwYZ87Irx8QQmDBggXQ6XSwsbFBt27dcOLECdmYgoICTJw4Ea6urrCzs8PAgQNx5crfh8qTkpLw2muvwcfHBzY2NnjiiScwf/58FBbK/0K6fPkyBgwYADs7O7i6umLSpEkGY+Lj4+Hv7w8bGxvUq1cPCxcu5DwzRFTpzMxUOPd+X2hsLKXazTuFaPteGE5ey+bfQ0QmQtFgFRkZifHjxyM6OhphYWEoLi5GYGAgcnL+nnF46dKlWL58OVatWoWYmBhotVoEBATg9u3b0pjJkydj8+bNCA0Nxf79+3Hnzh30798fJSUlAIDTp09Dr9fj888/x4kTJ/Dxxx9jzZo1mD17trSOkpISBAUFIScnB/v370doaCg2bdqEqVOnSmOys7MREBAAnU6HmJgYrFy5EsuWLcPy5curYG8RUW1naW6Gg7N7Yqz/E7J6vxV/8qJ2IlMhTEhaWpoAICIjI4UQQuj1eqHVasWSJUukMfn5+UKj0Yg1a9YIIYTIzMwUlpaWIjQ0VBpz9epVYWZmJnbs2FHmtpYuXSp8fHyk59u2bRNmZmbi6tWrUm3Dhg1CrVaLrKwsIYQQq1evFhqNRuTn50tjQkJChE6nE3q9vlzvMSsrSwCQ1klEVFF6vV40eOcPg0fItlNKt0ZUY5X3+9ukrrHKysoCALi4uAAAEhMTkZqaisDAv68tUKvV8Pf3x4EDBwAAR44cQVFRkWyMTqeDr6+vNKasbd3bDgBERUXB19cXOp1OqvXu3RsFBQU4cuSINMbf3x9qtVo25tq1a0hKSip1OwUFBcjOzpY9iIgeh0qlQtKSIHw1sp2svibyAnadSFWoKyICTOjidSEEpkyZgmeeeQa+vr4AgNTUu39BeHh4yMZ6eHhIy1JTU2FlZQVnZ+cyx9zvwoULWLlyJcaOHSvVUlNTDbbj7OwMKysr2bZK6+Wfvd4vJCQEGo1Genh5eZW9E4iIKqBXCw/EL5Bf1P7Gd0cw/vujCnVERCYTrCZMmIDjx49jw4YNBstUKpXsuRDCoHa/ssZcu3YNffr0wYsvvojXX3/9gdspbT2l9VLWawFg1qxZyMrKkh7JyckP7JuIqCIcrC2xZ6o/tI7WUm1rfAre+Pawgl0R1V4mEawmTpyILVu2YO/evfD09JTqWq0WgOHRoLS0NOlIkVarRWFhITIyMsocc8+1a9fQvXt3dO7cGV988YVsmVarNdhORkYGioqKZNsqrRfA8KjaPWq1Go6OjrIHEZExPeFmjwMze2BIu7///tx18jq8Z27FJ7vPKtgZUe2jaLASQmDChAn45ZdfEB4eDh8fH9lyHx8faLVahIWFSbXCwkJERkaiS5cuAAA/Pz9YWlrKxqSkpCAhIUEaAwBXr15Ft27d0LZtW6xduxZmZvK33rlzZyQkJCAlJUWq7dq1C2q1Gn5+ftKYffv2yaZg2LVrF3Q6Hby9vR9/hxARPSIzMxWWvtAa2yZ1ldU/2X0OPxy8rFBXRLWPosFq/PjxWL9+PX744Qc4ODggNTUVqampyMvLA3D39NrkyZOxePFibN68GQkJCQgODoatrS2GDRsGANBoNHjttdcwdepU7NmzB7GxsXjllVfQsmVL9OrVC8DdI1XdunWDl5cXli1bhhs3bkjbuicwMBAtWrTAiBEjEBsbiz179mDatGkYM2aMdJRp2LBhUKvVCA4ORkJCAjZv3ozFixdjypQpDz01SURUFVroHDG7XzNZbfbmeISfvo6C4hKFuiKqRSr994kPAKDUx9q1a6Uxer1ezJ8/X2i1WqFWq8Wzzz4r4uPjZevJy8sTEyZMEC4uLsLGxkb0799fXL58WVq+du3aMrf1T5cuXRJBQUHCxsZGuLi4iAkTJsimVhBCiOPHj4uuXbsKtVottFqtWLBgQbmnWhCC0y0QUdU4cTXLYDqGpTs4HQPRoyrv9zfvFVjFeK9AIqoq2+NT8OZ9vxD0reeIn8d2gbWluUJdEVVPvFcgEVEt17dlXfw5o7uslnA1G83e3YG8Qp4WJKoMDFZERDWYl4stTr/Xx6DefN4OXMnIVaAjopqNwYqIqIaztjTHxcX9sHDQk7L6Mx/sxZZj15CdX6RQZ0Q1D4MVEVEtYGamwohODbDu1fay+qQNsZjx03GFuiKqeRisiIhqCZVKhW5N3fHN6A6y+o4TqZjyYxyKSvQKdUZUczBYERHVMv5N3BD6RidZ7ZfYq2g8ZzvDFdFjYrAiIqqFOjWsgyNzexnUQ7adRkZOYSmvIKLyYLAiIqql6tirEdhCfp/T//2ViIGf7kcxj1wRPRIGKyKiWuyLke1w4j+90drLSaolp+eh0ZztuM1fCxJVGIMVEVEtZ6e2wK/juhjUZ/zMXwsSVRSDFRERQaVS4eLifni5g5dU256QCu+ZW3nzZqIKYLAiIiIAd+e6Cnm+FSb2aCSrN527A1/su6BQV0TVC4MVERHJTAloAmdbS1lt8bbTyC/ikSuih2GwIiIiGZVKhdh5gQb1Zu/uQG5hsQIdEVUfDFZERFSqD/7V0qA2Z3MChBAKdENUPTBYERFRqYa2r48Li/uhjp2VVNscexU+s7Yhp4BHrohKw2BFRERlMjdT4ci7AXi+bT1Z/cn5O5GcnqtQV0Smi8GKiIge6v3BLdG1saus1nXpXtzhkSsiGQYrIiJ6KBsrc3w7ugNe8POU1X3n78TGw8kKdUVkehisiIioXFQqFZa92BrDO9aX1Wf8fBwt5+/En+duKNQZkelgsCIiogp5/7mWmNW3max2u6AYI74+pFBHRKaDwYqIiCqsa2O3Uut5hZxElGo3BisiIqqwFjpHvDfoSdhYmsvqzeftYLiiWk0lONNblcrOzoZGo0FWVhYcHR2VboeI6LEIIXAoMR1Dv4iW1Ud2boCFg3wV6orI+Mr7/c0jVkRE9MhUKhU6NqyDuHkBsvq3UZeweNsp6PX8f3eqXRisiIjosTnZWhncAueLfRex6+R1hToiUsZjB6uSkhLExcUhIyPDGP0QEVE1NbR9fRx9V37kauz6I5izOV6hjoiqXoWD1eTJk/H1118DuBuq/P390bZtW3h5eSEiIsLY/RERUTXiYmeFbZO6ymrfH7yMHh9FICOnUKGuiKpOhYPVzz//jNatWwMAfv/9dyQmJuL06dOYPHky5syZY/QGiYioemmhM7yw9+KNHLz7W4IC3RBVrQoHq5s3b0Kr1QIAtm3bhhdffBFNmjTBa6+9hvh4Hu4lIiJg+ZDWBrU/jqfgw52nFeiGqOpUOFh5eHjg5MmTKCkpwY4dO9CrVy8AQG5uLszNzR/yaiIiqg2eb+uJxJB+aO2pkdU/3XsBIdtOKdQVUeWrcLB69dVXMWTIEPj6+kKlUiEg4O6FigcPHkSzZs0e8moiIqotVCoVNo7tjDe7PSGrf77vIr45kAROo0g10SNNEPrzzz8jOTkZL774Ijw9797p/JtvvoGTkxMGDRpk9CZrEk4QSkS1kRACPrO2yWofvtAKL7bzUqgjooop7/e3UWZez8zMhJOT0+OuplZgsCKi2io5PRddl+6V1d7t3wKvPeOjUEdE5VdpM69/8MEH+PHHH6XnQ4YMQZ06deDp6Ynjx48/WrdERFTjebnYYvcUf1ntvT9OYuvxFIU6IjK+Cgerzz//HF5edw/dhoWFISwsDNu3b0efPn0wbdo0ozdIREQ1RyN3e5xa2EdWG//DUXwXlaRMQ0RGVuFglZKSIgWrP/74A0OGDEFgYCBmzJiBmJgYozdIREQ1i42VOSKmdZPV3v3tBIpK9Mo0RGREFQ5Wzs7OSE5OBgDZdAtCCJSUlBi3OyIiqpG8Xe3ww5iOslrjOdtxPu22Qh0RGUeFg9Xzzz+PYcOGISAgALdu3ULfvn0BAHFxcWjUqJHRGyQiopqpyxOu8K0nvwi41/J9SM3KV6gjosdX4WD18ccfY8KECWjRogXCwsJgb28P4O4pwnHjxhm9QSIiqrm+f70TXvTzlNU6hezhaUGqtowy3QKVH6dbICIy9GPMZbyzSX5btBUvP4WBrXUKdUQkV97vb4tHWfmFCxfwySef4NSpU1CpVGjevDkmT56Mhg0bPnLDRERUew1tXx9P6jTov3K/VJu0IRZW5ir08a2rYGdEFVPhU4E7d+5EixYtcOjQIbRq1Qq+vr44ePCgdGqQiIjoUfjW0xjUxq4/ijWRF3j7G6o2Knwq8KmnnkLv3r2xZMkSWX3mzJnYtWsXjh49atQGaxqeCiQierDxPxw1mDT0/ed8MbxjA4U6IqrEW9pYW1sjPj4ejRs3ltXPnj2LVq1aIT+fv+Z4EAYrIqKH2x6fgje/l/+P+vIhrfF8W88yXkFUuSrtljZubm6Ii4szqMfFxcHd3b2iqyMiIjLQt6XhdVVTNh5ToBOiiqnwxetjxozBG2+8gYsXL6JLly5QqVTYv38/PvjgA0ydOrUyeiQiolrouafqYXPsVVltzuZ4vP9cS4U6Inq4Cp8KFELgk08+wUcffYRr164BAHQ6HaZPn4633nqrUpqsSXgqkIio/IQQ8Jm1zaD+7egOeLaJmwIdUW1VaddY/dPt23dvPeDg4ICcnBwcOXIEzz777KOurlZgsCIiqjjvmVsNaklLghTohGqrSrvG6p8cHBzg4OAAADh//jy6d+/+OKsjIiIqVWJIP4Pa3F/jkVfIe9SSaXmsYEVERFQVVCoVIqZ1k9XWR1/GJ7vPKtMQURkYrIiIqFrwdrVDYkg/dPBxkWqf77uISRtiFeyKSI7BioiIqg2VSoVvR3eQ1bYcu4b10ZcU6ohIrtzTLWzZsuWByxMTEx+7GSIiooextjTHr+OfxuBP/5Jqc39NQEM3O3R5wlXBzogq8KtAM7OHH9xSqVQoKeGFhA/CXwUSERnHnYJi+M7fKautGvYU+rfSKdQR1WRG/1WgXq9/6IOhioiIqoq92gJ/THxGVpvwA6+3ImUpeo1VSEgI2rdvDwcHB7i7u2Pw4ME4c+aMbIwQAgsWLIBOp4ONjQ26deuGEydOyMYUFBRg4sSJcHV1hZ2dHQYOHIgrV67IxmRkZGDEiBHQaDTQaDQYMWIEMjMzZWMuX76MAQMGwM7ODq6urpg0aRIKCwtlY+Lj4+Hv7w8bGxvUq1cPCxcu5F3XiYgU4ltPgxP/6S2rec/ciuISvUIdUW2naLCKjIzE+PHjER0djbCwMBQXFyMwMBA5OTnSmKVLl2L58uVYtWoVYmJioNVqERAQIE1OCgCTJ0/G5s2bERoaiv379+POnTvo37+/7AjasGHDEBcXhx07dmDHjh2Ii4vDiBEjpOUlJSUICgpCTk4O9u/fj9DQUGzatEl2m57s7GwEBARAp9MhJiYGK1euxLJly7B8+fJK3lNERFQWO7UFzizqI6s1mrMdBy/eUqgjqtWECUlLSxMARGRkpBBCCL1eL7RarViyZIk0Jj8/X2g0GrFmzRohhBCZmZnC0tJShIaGSmOuXr0qzMzMxI4dO4QQQpw8eVIAENHR0dKYqKgoAUCcPn1aCCHEtm3bhJmZmbh69ao0ZsOGDUKtVousrCwhhBCrV68WGo1G5OfnS2NCQkKETqcTer2+XO8xKytLAJDWSURExvHS51GiwTt/yB7nrmcr3RbVEOX9/jap6RaysrIAAC4ud+coSUxMRGpqKgIDA6UxarUa/v7+OHDgAADgyJEjKCoqko3R6XTw9fWVxkRFRUGj0aBjx47SmE6dOkGj0cjG+Pr6Qqf7+6LH3r17o6CgAEeOHJHG+Pv7Q61Wy8Zcu3YNSUlJpb6ngoICZGdnyx5ERGR8G97oZFDrtXwfIs6kKdAN1VYmE6yEEJgyZQqeeeYZ+Pr6AgBSU1MBAB4eHrKxHh4e0rLU1FRYWVnB2dn5gWPc3d0Ntunu7i4bc/92nJ2dYWVl9cAx957fG3O/kJAQ6boujUYDLy+vh+wJIiJ6VGcX9YWZSl77ZPc5lOh5LSxVjXIHq0OHDsmuWRL3XbBdUFCAjRs3PnIjEyZMwPHjx7FhwwaDZSqV/L8SIYRB7X73jyltvDHG3NsPZfUza9YsZGVlSY/k5OQH9k1ERI/OysIM0bN6ympxyZlotygMJ6/xjAFVvnIHq86dO+PWrb8vBNRoNLh48aL0PDMzEy+//PIjNTFx4kRs2bIFe/fuhaenp1TXarUADI8GpaWlSUeKtFotCgsLkZGR8cAx169fN9jujRs3ZGPu305GRgaKiooeOCYt7e4h5vuPZN2jVqvh6OgoexARUeVxc1Cjz5NaWS0jtwj9VvzJX3FTpSt3sLr/w1jah7OiH1ghBCZMmIBffvkF4eHh8PHxkS338fGBVqtFWFiYVCssLERkZCS6dOkCAPDz84OlpaVsTEpKChISEqQxnTt3RlZWFg4dOiSNOXjwILKysmRjEhISkJKSIo3ZtWsX1Go1/Pz8pDH79u2TTcGwa9cu6HQ6eHt7V+i9ExFR5VCpVFgzwg9H5vYyWLbp6FUFOqLaxKjXWD3s9Nz9xo8fj/Xr1+OHH36Ag4MDUlNTkZqairy8PGl9kydPxuLFi7F582YkJCQgODgYtra2GDZsGIC7R85ee+01TJ06FXv27EFsbCxeeeUVtGzZEr163f2Pqnnz5ujTpw/GjBmD6OhoREdHY8yYMejfvz+aNm0KAAgMDESLFi0wYsQIxMbGYs+ePZg2bRrGjBkjHWUaNmwY1Go1goODkZCQgM2bN2Px4sWYMmVKhd87ERFVrjr2aiSG9JPVpv10DLGXM8p4BZERlPdnhiqVSly/fl16bm9vLy5cuCA9T01NFWZmZuVdnRB3D2+V+li7dq00Rq/Xi/nz5wutVivUarV49tlnRXx8vGw9eXl5YsKECcLFxUXY2NiI/v37i8uXL8vG3Lp1SwwfPlw4ODgIBwcHMXz4cJGRkSEbc+nSJREUFCRsbGyEi4uLmDBhgmxqBSGEOH78uOjatatQq9VCq9WKBQsWlHuqBSE43QIRUVUb//0Rg2kYUjLzlG6Lqpnyfn9X6F6B4eHh0lQIXbp0wcaNG6Vrom7evImAgADe1uYheK9AIqKqJYRAvxX7cSpFfvF60pIghTqi6qi8398VClYqlarU66ju1XkT5odjsCIiqnpLd5zG6ogLBvVdbz+LJh4OCnRE1U15v78tyrvCxMREozRGRERU1Sb2aIzP9100mM8q8ON9PHJFRlXuI1ZkHDxiRUSknILiEuxISMVboXGy+s7Jz6KplkeuqGzl/f4u968C09PTceXKFVntxIkTePXVVzFkyBD88MMPj94tERFRFVBbmGNQm3r4X3A7WX3R1pOc44qMotzBavz48Vi+fLn0PC0tDV27dkVMTAwKCgoQHByM7777rlKaJCIiMqYezeSTOv957iaeW31AoW6oJil3sIqOjsbAgQOl599++y1cXFwQFxeH3377DYsXL8ann35aKU0SEREZW6eGLrLnccmZWLbzjELdUE1R7mCVmpoqmxk9PDwczz33HCws7l7/PnDgQJw7d874HRIREVWCVcPaor6Lrby29zzOXr+tUEdUE5Q7WDk6OiIzM1N6fujQIXTq1El6rlKpUFBQYNTmiIiIKourvRr7ZnRHPScbWT3w433YkZBaxquIHqzcwapDhw5YsWIF9Ho9fv75Z9y+fRs9evSQlp89exZeXl6V0iQREVFl+WtmD3Twlp8WHLv+CE5cy1KoI6rOyh2s3nvvPfz222+wsbHB0KFDMWPGDDg7O0vLQ0ND4e/vXylNEhERVaZ1o9ujo488XAWt2I99Z28o1BFVVxWax+rGjRs4cOAAtFotOnbsKFu2detWtGjRQnYdFhniPFZERKbpSkYunvlgr0E9cno3NKhjp0BHZEqMfksbMg4GKyIi03XzTgHaLdotqzlaWyBuXiDMzFQKdUWmwOjB6ttvvy3XhkeOHFm+DmspBisiItN2Pu0Oei2PNKgfmdsLdezVCnREpqBSbsJsb28PCwuLMmenValUSE9Pf7SOawkGKyIi0+c9c6tBLaCFB74c2a6U0VQbGP2WNs2bN4eVlRVGjhyJyMhIZGRkGDwYqoiIqCaImNbNoBZ28rrBTZyJ7lfuYHXixAls3boVeXl5ePbZZ9GuXTt89tlnyM7Orsz+iIiIqpy3qx0uLu5nMIHoE7O3KdQRVRflDlYA0LFjR3z++edISUnBpEmTsHHjRtStWxfDhw/n5KBERFSjmJmp8PvEZwzqEzfEKtANVRcVClb32NjYYOTIkfjPf/6DDh06IDQ0FLm5ucbujYiISFEaG0tsnSQPV78fu4ZPdp9VqCMydRUOVlevXsXixYvRuHFjvPTSS2jfvj1OnDghmyyUiIiopnhSp8H59/vKap/sPoeZm44r1BGZsnIHq40bN6Jv375o3LgxYmJi8NFHHyE5ORlLly5Fs2bNKrNHIiIiRVmYm8HNQT7VQmhMMpLTebaG5Co03UL9+vUxfPhweHh4lDlu0qRJRmuuJuJ0C0RE1VNOQTGenL/ToJ4Y0g8qFScPremMPo+Vt7f3Qz84KpUKFy9erFintQyDFRFR9RV7OQPPrT4gqw1qo8PIzt7wa8BLYmoy3tLGRDFYERFVb0II+MwynHZh95Rn0cjdQYGOqCoYfYLQ8rh69aoxV0dERGRyVCoVomb1MKj3Wr6P11yRcYJVamoqJk6ciEaNGhljdURERCatrsYGSUuCDOqDPv1LgW7IlJQ7WGVmZmL48OFwc3ODTqfDihUroNfrMW/ePDRs2BDR0dH43//+V5m9EhERmZT5A1rInqfnFOLFNQfKGE21QbmD1ezZs7Fv3z6MGjUKLi4uePvtt9G/f3/s378f27dvR0xMDF5++eXK7JWIiMikjOrsbVCLScrA1cy8qm+GTEK5g9XWrVuxdu1aLFu2DFu2bIEQAk2aNEF4eDj8/f0rs0ciIiKTZGamwvTeTQ3qTy8Jx/ErmVXfECmu3MHq2rVraNHi7iHPhg0bwtraGq+//nqlNUZERFQdjO/eCM3rGv5KbPS6GAW6IaWVO1jp9XpYWlpKz83NzWFnZ1cpTREREVUnX470wwt+nrLazTuFuHDjjkIdkVIqNPN63759oVbfndL/999/R48ePQzC1S+//GL8LmsQzmNFRFRzxSVnYvA/fhloa2WOg7N7wsHa8gGvourA6PNYjRo1Cu7u7tBoNNBoNHjllVeg0+mk5/ceREREtVUbLydM7tVYep5bWIKWC3Zh75k0BbuiqsSZ16sYj1gREdVseYUlaD5vh0G9tHmvqPpQZOZ1IiKi2s7GyhzrXm1vcO/AgOWRCnVEVYnBioiIyMi6NXXHT//ujOeeqifVzqXdwX93n1OwK6oKDFZERESVwMxMhXf6NJPVPt59Fq98dRC8CqfmYrAiIiKqJFqNNc6931dW23/+Jvqv3K9QR1TZGKyIiIgqkaW54VftiWvZPGpVQzFYERERVbL1r3U0qH2x7yKKSvQKdEOVicGKiIiokj3T2BWfDG0jq4VsP40n5+1EYTHDVU3CYEVERFQFBrbW4fVnfGS1whI94pIzlWmIKgWDFRERURUwM1Nhbv8W+P51+WnBV746qFBHVBkYrIiIiKrQ041cZc8LS/TILSxWqBsyNgYrIiKiKvbN6A6y5y3m7cQrXx1Eek6hQh2RsTBYERERVTH/Jm74cmQ7WW3/+Zto+16YQh2RsTBYERERKSCghUep9WJOwVCtMVgREREpJPSNTga1DTHJCnRCxsJgRUREpJBODevg3/4NZbV3f01A5NkbCnVEj4vBioiISEFv92qCT4e1ldVG/e8Q2r+/G0k3cxTqih4VgxUREZGCrC3NEdSqLp5uVEdWv3G7AIu2nlKoK3pUDFZEREQmYNmLreHuoJbVdp+6jv3nbirUET0KBisiIiITUFdjg7Ap/gb1V74+iP/tT1SgI3oUDFZEREQmQmNjiYT/9Maiwb6y+sI/TkKvFwp1RRXBYEVERGRC7NUWeKVTA4N7CuYVlSjUEVUEgxUREZEJerqRK7o1dZOed1sWoVwzVG4MVkRERCZq1T+mYbhxuwB7Tl3HpVucgsGUKRqs9u3bhwEDBkCn00GlUuHXX3+VLb9+/TqCg4Oh0+lga2uLPn364Ny5c7IxFy5cwHPPPQc3Nzc4OjpiyJAhuH79umzM2bNnMWjQILi6usLR0RFPP/009u7dKxtz+fJlDBgwAHZ2dnB1dcWkSZNQWCi/GWZ8fDz8/f1hY2ODevXqYeHChRCC57yJiKhy2Kst4OViIz1/7ZvD8P8wgtdbmTBFg1VOTg5at26NVatWGSwTQmDw4MG4ePEifvvtN8TGxqJBgwbo1asXcnJypNcHBgZCpVIhPDwcf/31FwoLCzFgwADo9X/faykoKAjFxcUIDw/HkSNH0KZNG/Tv3x+pqakAgJKSEgQFBSEnJwf79+9HaGgoNm3ahKlTp0rryM7ORkBAAHQ6HWJiYrBy5UosW7YMy5cvr+S9REREtVnktO4GtYazt+F2fpEC3dBDCRMBQGzevFl6fubMGQFAJCQkSLXi4mLh4uIivvzySyGEEDt37hRmZmYiKytLGpOeni4AiLCwMCGEEDdu3BAAxL59+6Qx2dnZAoDYvXu3EEKIbdu2CTMzM3H16lVpzIYNG4RarZbWvXr1aqHRaER+fr40JiQkROh0OqHX68v9PrOysgQAWc9EREQPknjjjmjwzh+yx/5zN5Ruq1Yp7/e3yV5jVVBQAACwtraWaubm5rCyssL+/fulMSqVCmr13xOqWVtbw8zMTBpTp04dNG/eHN9++y1ycnJQXFyMzz//HB4eHvDz8wMAREVFwdfXFzqdTlpP7969UVBQgCNHjkhj/P39Zdvq3bs3rl27hqSkpAe+j+zsbNmDiIioIrxd7XByYW9ZbeT/DuGP49cU6ojKYrLBqlmzZmjQoAFmzZqFjIwMFBYWYsmSJUhNTUVKSgoAoFOnTrCzs8M777yD3Nxc5OTkYPr06dDr9dIYlUqFsLAwxMbGwsHBAdbW1vj444+xY8cOODk5AQBSU1Ph4eEh276zszOsrKyk04Wljbn3/N6Y0oSEhECj0UgPLy8vo+wfIiKqXWytLGTPS/QCE36IxXOr/8LxK5nKNEUGTDZYWVpaYtOmTTh79ixcXFxga2uLiIgI9O3bF+bm5gAANzc3/PTTT/j9999hb28PjUaDrKwstG3bVhojhMC4cePg7u6OP//8E4cOHcKgQYPQv39/KXwBdwPY/YQQsvr9Y8T/X7he2mvvmTVrFrKysqRHcnLyo+8UIiKq1X6f8IxBLfZyJkb975AC3VBpLB4+RDl+fn6Ii4tDVlYWCgsL4ebmho4dO6Jdu3bSmMDAQFy4cAE3b96EhYUFnJycoNVq4ePjAwAIDw/HH3/8gYyMDDg6OgIAVq9ejbCwMHzzzTeYOXMmtFotDh48KNt2RkYGioqKpKNSWq3W4MhUWloaABgcyfontVotO31IRET0qFp6ahA+1R89PoqU1TNyeSG7qTDZI1b/pNFo4ObmhnPnzuHw4cMYNGiQwRhXV1c4OTkhPDwcaWlpGDhwIAAgNzcXAGBmJn+rZmZm0i8HO3fujISEBNkRrF27dkGtVkvXYXXu3Bn79u2TTcGwa9cu6HQ6eHt7G/X9EhERlaWhmz0WDnrSoF5YrC9lNFU1RYPVnTt3EBcXh7i4OABAYmIi4uLicPnyZQDATz/9hIiICGnKhYCAAAwePBiBgYHSOtauXYvo6GhcuHAB69evx4svvoi3334bTZs2BXA3EDk7O2PUqFE4duwYzp49i+nTpyMxMRFBQUEA7h71atGiBUaMGIHY2Fjs2bMH06ZNw5gxY6SjXMOGDYNarUZwcDASEhKwefNmLF68GFOmTHngqUAiIiJjG9nZ26DWZO72qm+EDFXFTxTLsnfvXgHA4DFq1CghhBD//e9/haenp7C0tBT169cXc+fOFQUFBbJ1vPPOO8LDw0NYWlqKxo0bi48++shg+oOYmBgRGBgoXFxchIODg+jUqZPYtm2bbMylS5dEUFCQsLGxES4uLmLChAmyqRWEEOL48eOia9euQq1WC61WKxYsWFChqRaE4HQLRERkHHmFxQZTMDR45w9x4PxNpVurkcr7/a0SglOHV6Xs7GzpIvt7R8OIiIgeRW5hMVrM2ymrOagtEP+f3mW8gh5Veb+/q8U1VkRERGTI1soCnw1vK6vdLihGflGJQh0RgxUREVE11rdlXQxuo5PVmr27AyevcUJqJTBYERERVXMfD20DWytzWa3fij9x4lqWQh3VXgxWRERE1ZxKpcKBmT0M6mEnryvQTe3GYEVERFQDONla4eyivrLaJ7vPKdRN7cVgRUREVENYWRh+rR9OSlegk9qLwYqIiKgG8W/iJnv+wpoofLL7LNJu5yvUUe3CeayqGOexIiKiyiaEgM+sbbKal4sN/pxheB0WlQ/nsSIiIqqlVCoVVt83v1Vyeh6KSng/wcrGYEVERFQD9WtZ16D2y9ErCnRSuzBYERER1VDH5gXKnr+zKR5bjl1TqJvagcGKiIiohtLYWqKDj4usNmlDLD7de16hjmo+BisiIqIa7L1Bvga1D3eeQW5hsQLd1HwMVkRERDVYU60DkpYEoUVd+S/ZNh3h9VaVgcGKiIioFtj2Vle84OcpPX/3txPgjEvGx2BFRERUS8zq20z23GfWNpToGa6MicGKiIiolqhjr8aut5+V1Z6YvQ3jvj+iUEc1D4MVERFRLdLEw8Ggti0+FVm5RQp0U/MwWBEREdUyP43tbFDL4a8EjYLBioiIqJZp7+1icMubLkvCceJalkId1RwMVkRERLXQ041cDWpzNico0EnNwmBFRERUC2lsLJG0JEhWi0vOxItrDijUUc3AYEVERFSLHZzdU/Y8JikDZ1JvK9RN9cdgRUREVIt5OFpjYo9GslrvT/ahuESvUEfVG4MVERFRLdfGy8mg9tX+xKpvpAZgsCIiIqrlejRzx+LnWspqS7afxi9HeT/BimKwIiIiquVUKhWGdayPjf+Wz281ZeMxJKfnKtRV9cRgRURERACADj4usDRXyWpdl+5FRk6hQh1VPwxWREREJPliRDuD2oBV+yEEb9ZcHgxWREREJOnezB1dG8snD72SkQefWdtwNTNPoa6qDwYrIiIikvnutY54UudoUH96STiPXD0EgxUREREZ2DzuaTR0tTOo5xWVKNBN9cFgRURERAasLMzwxUjD663SeSH7AzFYERERUam8XGxga2Uuqz3zwV4cTkpXqCPTx2BFREREpVJbmOPg7J5YG9xeVn9hTRSOJWeiiLe9McBgRURERGVysLZE92buBvVBn/6Fhb+fVKAj08ZgRURERA/V5Yk6BrXvoi8p0IlpY7AiIiKih7r/XoL3ZObyYvZ/YrAiIiKih/J2tUPSkiCDepuFYcjnFAwSBisiIiJ6LOt5SlDCYEVERETl9ueM7tDYWMpqi7aeUqgb08NgRUREROXm5WKLg7N7GtS9Z27F69/EKNCRaWGwIiIiogqxtjQv9Xqr3afScPNOgQIdmQ4GKyIiInok/VpqDWq1/ZY3DFZERET0SFYP98PLHbxktbm/JqCguPb+SpDBioiIiB7ZosEt8a+2ntLzQ4npCNl2GqlZ+Qp2pRwGKyIiInpk5mYqfDSktay27kASOoXsUagjZTFYERER0WOrq7E2qOUUFCvQibIYrIiIiOix7Z3WDS3qOspqqyPOI6OWXczOYEVERESPzdrSHF8Ht5PVPt17AWO+PaxQR8pgsCIiIiKjqKuxMagdvpSBxJs5CnSjDAYrIiIiMpolz7c0qHVfFoE7teR6KwYrIiIiMpqXOtTHG882NKifu35bgW6qHoMVERERGdW0wKYGNb0QCnRS9RisiIiIyKisLMwQM6eXrPavz6IQeuiyQh1VHQYrIiIiMjo3BzVO/Ke3rDbzl3hsOXZNoY6qhqLBat++fRgwYAB0Oh1UKhV+/fVX2fLr168jODgYOp0Otra26NOnD86dOycbc+HCBTz33HNwc3ODo6MjhgwZguvXrxtsa+vWrejYsSNsbGzg6uqK559/Xrb88uXLGDBgAOzs7ODq6opJkyahsFA+90Z8fDz8/f1hY2ODevXqYeHChRC15NAmERFRRdmpLQxqkzbE4re4qwp0UzUUDVY5OTlo3bo1Vq1aZbBMCIHBgwfj4sWL+O233xAbG4sGDRqgV69eyMnJkV4fGBgIlUqF8PBw/PXXXygsLMSAAQOg1+uldW3atAkjRozAq6++imPHjuGvv/7CsGHDpOUlJSUICgpCTk4O9u/fj9DQUGzatAlTp06VxmRnZyMgIAA6nQ4xMTFYuXIlli1bhuXLl1fiHiIiIqrehnesb1B7KzSu6hupKsJEABCbN2+Wnp85c0YAEAkJCVKtuLhYuLi4iC+//FIIIcTOnTuFmZmZyMrKksakp6cLACIsLEwIIURRUZGoV6+e+Oqrr8rc9rZt24SZmZm4evWqVNuwYYNQq9XSulevXi00Go3Iz8+XxoSEhAidTif0en2532dWVpYAIOuZiIiopsorLBbHkzNFh/fDRIN3/pAeyek5SrdWIeX9/jbZa6wKCgoAANbWf997yNzcHFZWVti/f780RqVSQa1WS2Osra1hZmYmjTl69CiuXr0KMzMzPPXUU6hbty769u2LEydOSK+JioqCr68vdDqdVOvduzcKCgpw5MgRaYy/v79sW71798a1a9eQlJT0wPeRnZ0texAREdUW1pbmaOmpQcS07rL6Mx/sVaijymWywapZs2Zo0KABZs2ahYyMDBQWFmLJkiVITU1FSkoKAKBTp06ws7PDO++8g9zcXOTk5GD69OnQ6/XSmIsXLwIAFixYgLlz5+KPP/6As7Mz/P39kZ6eDgBITU2Fh4eHbPvOzs6wsrJCampqmWPuPb83pjQhISHQaDTSw8vLywh7h4iIqHqxsTI3qPl/uBd6fc26Vtlkg5WlpSU2bdqEs2fPwsXFBba2toiIiEDfvn1hbn73X46bmxt++ukn/P7777C3t4dGo0FWVhbatm0rjbl3rdWcOXPwr3/9C35+fli7di1UKhV++uknaXsqlcqgByGErH7/GPH/F66X9tp7Zs2ahaysLOmRnJz8iHuEiIioetv+VlfZ80u3cnHx5h2FuqkchpfrmxA/Pz/ExcUhKysLhYWFcHNzQ8eOHdGu3d83eQwMDMSFCxdw8+ZNWFhYwMnJCVqtFj4+PgCAunXrAgBatGghvUatVqNhw4a4fPnufBparRYHDx6UbTsjIwNFRUXSUSmtVmtwZCotLQ0ADI5k/ZNarZadPiQiIqqtmtd1RF9fLbYn/P19WswjVlVPo9HAzc0N586dw+HDhzFo0CCDMa6urnByckJ4eDjS0tIwcOBAAHfDmVqtxpkzZ6SxRUVFSEpKQoMGDQAAnTt3RkJCgnT6EAB27doFtVoNPz8/acy+fftkUzDs2rULOp0O3t7elfG2iYiIapyXO8h/Jdjnkz+RU4PuI6hosLpz5w7i4uIQFxcHAEhMTERcXJx0JOmnn35CRESENOVCQEAABg8ejMDAQGkda9euRXR0NC5cuID169fjxRdfxNtvv42mTe9Op+/o6IixY8di/vz52LVrF86cOYM333wTAPDiiy8CuHvUq0WLFhgxYgRiY2OxZ88eTJs2DWPGjIGjoyMAYNiwYVCr1QgODkZCQgI2b96MxYsXY8qUKQ88FUhERER/e6aRq0G4enL+Tty8U6BQR0ZWJb9RLMPevXsFAIPHqFGjhBBC/Pe//xWenp7C0tJS1K9fX8ydO1cUFBTI1vHOO+8IDw8PYWlpKRo3biw++ugjg+kPCgsLxdSpU4W7u7twcHAQvXr1kk3jIIQQly5dEkFBQcLGxka4uLiICRMmyKZWEEKI48ePi65duwq1Wi20Wq1YsGBBhaZaEILTLRAREQkhZFMvNHjnDzHxh6NKt/RA5f3+VgnBqcOrUnZ2tnSR/b2jYURERLVN2u18dHh/j6x2ZlEfqC0Mfz1oCsr7/V0trrEiIiKimsXdwRpfjmwnqzWduwO3qvkpQQYrIiIiUkSv5u5YPbytrOa3aDey84sU6ujxMVgRERGRIlQqFfq1rGtQb7VglwLdGAeDFRERESnqrZ6NDWrVdQoGBisiIiJS1NsBTZAY0k9We3L+Tqz7K1Ghjh4dgxUREREprrQ5IRf8fhId3t+NguISBTp6NAxWREREZBL+1dbToJZ2uwAHzt9SoJtHw2BFREREJuHDF1qVWucRKyIiIqIKMjNT4dfxTxvUi0qqz1zmDFZERERkMtp4OWH00z6y2sQNscgvqh5HrRisiIiIyKRM793UoNbs3R346XCyAt1UDIMVERERmRQbK3NEzeoBB7WFrD795+Mw9VscM1gRERGRyamrscGaEX4G9YJivQLdlB+DFREREZmkLk/UwcDWOlktt9C0r7VisCIiIiKTpFKpsOLlp5C0JEiqtX0vDKsjzkOvN81TggxWREREVK0s3XEGW+NTlG6jVAxWREREZPL6t6orez5xQ6xJXsjOYEVEREQmb9Wwtga15PQ8BTp5MAYrIiIiqhYGtZFfyP7sh3tN7qgVgxURERFVC672aoPakUsZCnRSNgYrIiIiqhZefdrboDbtp2NV38gDMFgRERFRteDpbIu907rJakm3cjHyf4dM5pQggxURERFVGz6udtgz1R/PNHKVavvO3kCeidykmcGKiIiIqpUn3Ozx1ah2strUjaZxSpDBioiIiKoda0tz/DCmo/R8e0IqPtp1RsGO7mKwIiIiomqpyxOuaFDHVnq+Mvy84tdaMVgRERFRtRVx38XsqyMuKNPI/2OwIiIiompLpVKhZT2N9PzDnWeQlp2vWD8MVkRERFStfT7CT/bcydZKoU4YrIiIiKia0znZYPmQ1gCA0U/7wMpCuXhjodiWiYiIiIzk+baeeL6tp9Jt8IgVERERkbEwWBEREREZCYMVERERkZEwWBEREREZCYMVERERkZEwWBEREREZCYMVERERkZEwWBEREREZCYMVERERkZEwWBEREREZCYMVERERkZEwWBEREREZCYMVERERkZEwWBEREREZiYXSDdQ2QggAQHZ2tsKdEBERUXnd+96+9z1eFgarKnb79m0AgJeXl8KdEBERUUXdvn0bGo2mzOUq8bDoRUal1+tx7do1ODg4QKVSGW292dnZ8PLyQnJyMhwdHY223pqI+6r8uK8qhvur/Livyo/7qvwqc18JIXD79m3odDqYmZV9JRWPWFUxMzMzeHp6Vtr6HR0d+R9eOXFflR/3VcVwf5Uf91X5cV+VX2XtqwcdqbqHF68TERERGQmDFREREZGRMFjVEGq1GvPnz4darVa6FZPHfVV+3FcVw/1VftxX5cd9VX6msK948ToRERGRkfCIFREREZGRMFgRERERGQmDFREREZGRMFgRERERGQmDVQ2xevVq+Pj4wNraGn5+fvjzzz+VbqlKLViwACqVSvbQarXSciEEFixYAJ1OBxsbG3Tr1g0nTpyQraOgoAATJ06Eq6sr7OzsMHDgQFy5cqWq34rR7du3DwMGDIBOp4NKpcKvv/4qW26sfZORkYERI0ZAo9FAo9FgxIgRyMzMrOR3Z1wP21fBwcEGn7NOnTrJxtSWfRUSEoL27dvDwcEB7u7uGDx4MM6cOSMbw8/WXeXZV/xs3fXZZ5+hVatW0gSfnTt3xvbt26Xl1eIzJajaCw0NFZaWluLLL78UJ0+eFG+99Zaws7MTly5dUrq1KjN//nzx5JNPipSUFOmRlpYmLV+yZIlwcHAQmzZtEvHx8WLo0KGibt26Ijs7WxozduxYUa9ePREWFiaOHj0qunfvLlq3bi2Ki4uVeEtGs23bNjFnzhyxadMmAUBs3rxZttxY+6ZPnz7C19dXHDhwQBw4cED4+vqK/v37V9XbNIqH7atRo0aJPn36yD5nt27dko2pLfuqd+/eYu3atSIhIUHExcWJoKAgUb9+fXHnzh1pDD9bd5VnX/GzddeWLVvE1q1bxZkzZ8SZM2fE7NmzhaWlpUhISBBCVI/PFINVDdChQwcxduxYWa1Zs2Zi5syZCnVU9ebPny9at25d6jK9Xi+0Wq1YsmSJVMvPzxcajUasWbNGCCFEZmamsLS0FKGhodKYq1evCjMzM7Fjx45K7b0q3R8WjLVvTp48KQCI6OhoaUxUVJQAIE6fPl3J76pylBWsBg0aVOZrauu+EkKItLQ0AUBERkYKIfjZepD795UQ/Gw9iLOzs/jqq6+qzWeKpwKrucLCQhw5cgSBgYGyemBgIA4cOKBQV8o4d+4cdDodfHx88NJLL+HixYsAgMTERKSmpsr2kVqthr+/v7SPjhw5gqKiItkYnU4HX1/fGr0fjbVvoqKioNFo0LFjR2lMp06doNFoatz+i4iIgLu7O5o0aYIxY8YgLS1NWlab91VWVhYAwMXFBQA/Ww9y/766h58tuZKSEoSGhiInJwedO3euNp8pBqtq7ubNmygpKYGHh4es7uHhgdTUVIW6qnodO3bEt99+i507d+LLL79EamoqunTpglu3bkn74UH7KDU1FVZWVnB2di5zTE1krH2TmpoKd3d3g/W7u7vXqP3Xt29ffP/99wgPD8dHH32EmJgY9OjRAwUFBQBq774SQmDKlCl45pln4OvrC4CfrbKUtq8Afrb+KT4+Hvb29lCr1Rg7diw2b96MFi1aVJvPlMVjr4FMgkqlkj0XQhjUarK+fftKf27ZsiU6d+6MJ554At988410Aeij7KPash+NsW9KG1/T9t/QoUOlP/v6+qJdu3Zo0KABtm7diueff77M19X0fTVhwgQcP34c+/fvN1jGz5ZcWfuKn62/NW3aFHFxccjMzMSmTZswatQoREZGSstN/TPFI1bVnKurK8zNzQ1SdlpamkGqr03s7OzQsmVLnDt3Tvp14IP2kVarRWFhITIyMsocUxMZa99otVpcv37dYP03btyo0fuvbt26aNCgAc6dOwegdu6riRMnYsuWLdi7dy88PT2lOj9bhsraV6WpzZ8tKysrNGrUCO3atUNISAhat26N//73v9XmM8VgVc1ZWVnBz88PYWFhsnpYWBi6dOmiUFfKKygowKlTp1C3bl34+PhAq9XK9lFhYSEiIyOlfeTn5wdLS0vZmJSUFCQkJNTo/WisfdO5c2dkZWXh0KFD0piDBw8iKyurRu+/W7duITk5GXXr1gVQu/aVEAITJkzAL7/8gvDwcPj4+MiW87P1t4ftq9LU5s/W/YQQKCgoqD6fqce+/J0Ud2+6ha+//lqcPHlSTJ48WdjZ2YmkpCSlW6syU6dOFREREeLixYsiOjpa9O/fXzg4OEj7YMmSJUKj0YhffvlFxMfHi5dffrnUn+h6enqK3bt3i6NHj4oePXrUiOkWbt++LWJjY0VsbKwAIJYvXy5iY2Ol6TiMtW/69OkjWrVqJaKiokRUVJRo2bJltfqZtxAP3le3b98WU6dOFQcOHBCJiYli7969onPnzqJevXq1cl+9+eabQqPRiIiICNkUAbm5udIYfrbueti+4mfrb7NmzRL79u0TiYmJ4vjx42L27NnCzMxM7Nq1SwhRPT5TDFY1xKeffioaNGggrKysRNu2bWU/460N7s1lYmlpKXQ6nXj++efFiRMnpOV6vV7Mnz9faLVaoVarxbPPPivi4+Nl68jLyxMTJkwQLi4uwsbGRvTv319cvny5qt+K0e3du1cAMHiMGjVKCGG8fXPr1i0xfPhw4eDgIBwcHMTw4cNFRkZGFb1L43jQvsrNzRWBgYHCzc1NWFpaivr164tRo0YZ7Ifasq9K208AxNq1a6Ux/Gzd9bB9xc/W30aPHi19l7m5uYmePXtKoUqI6vGZUgkhxOMf9yIiIiIiXmNFREREZCQMVkRERERGwmBFREREZCQMVkRERERGwmBFREREZCQMVkRERERGwmBFREREZCQMVkRECoqIiIBKpUJmZqbSrRCRETBYERERERkJgxURERGRkTBYEVGtJoTA0qVL0bBhQ9jY2KB169b4+eefAfx9mm7r1q1o3bo1rK2t0bFjR8THx8vWsWnTJjz55JNQq9Xw9vbGRx99JFteUFCAGTNmwMvLC2q1Go0bN8bXX38tG3PkyBG0a9cOtra26NKlC86cOVO5b5yIKgWDFRHVanPnzsXatWvx2Wef4cSJE3j77bfxyiuvIDIyUhozffp0LFu2DDExMXB3d8fAgQNRVFQE4G4gGjJkCF566SXEx8djwYIFePfdd7Fu3Trp9SNHjkRoaChWrFiBU6dOYc2aNbC3t5f1MWfOHHz00Uc4fPgwLCwsMHr06Cp5/0RkXLwJMxHVWjk5OXB1dUV4eDg6d+4s1V9//XXk5ubijTfeQPfu3REaGoqhQ4cCANLT0+Hp6Yl169ZhyJAhGD58OG7cuIFdu3ZJr58xYwa2bt2KEydO4OzZs2jatCnCwsLQq1cvgx4iIiLQvXt37N69Gz179gQAbNu2DUFBQcjLy4O1tXUl7wUiMiYesSKiWuvkyZPIz89HQEAA7O3tpce3336LCxcuSOP+GbpcXFzQtGlTnDp1CgBw6tQpPP3007L1Pv300zh37hxKSkoQFxcHc3Nz+Pv7P7CXVq1aSX+uW7cuACAtLe2x3yMRVS0LpRsgIlKKXq8HAGzduhX16tWTLVOr1bJwdT+VSgXg7jVa9/58zz9PBNjY2JSrF0tLS4N13+uPiKoPHrEiolqrRYsWUKvVuHz5Mho1aiR7eHl5SeOio6OlP2dkZODs2bNo1qyZtI79+/fL1nvgwAE0adIE5ubmaNmyJfR6veyaLSKquXjEiohqLQcHB0ybNg1vv/029Ho9nnnmGWRnZ+PAgQOwt7dHgwYNAAALFy5EnTp14OHhgTlz5sDV1RWDBw8GAEydOhXt27fHe++9h6FDhyIqKgqrVq3C6tWrAQDe3t4YNWoURo8ejRUrVqB169a4dOkS0tLSMGTIEKXeOhFVEgYrIqrV3nvvPbi7uyMkJAQXL16Ek5MT2rZti9mzZ0un4pYsWYK33noL586dQ+vWrbFlyxZYWVkBANq2bYuNGzdi3rx5eO+991C3bl0sXLgQwcHB0jY+++wzzJ49G+PGjcOtW7dQv359zJ49W4m3S0SVjL8KJCIqw71f7GVkZMDJyUnpdoioGuA1VkRERERGwmBFREREZCQ8FUhERERkJDxiRURERGQkDFZERERERsJgRURERGQkDFZERERERsJgRURERGQkDFZERERERsJgRURERGQkDFZERERERsJgRURERGQk/wdju2Q50Za0nwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(epochs), [loss.detach().cpu().numpy() for loss in final_losses])\n",
    "plt.ylabel('RMSE Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.title('Training Loss Curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fbbb9acd-510e-4088-8a27-3d348f735783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 189959.15625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([180])) that is different to the input size (torch.Size([180, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "y_pred=\"\"\n",
    "with torch.no_grad():\n",
    "    y_pred=model(test_catg,test_cont)\n",
    "    loss=torch.sqrt(loss_funcation(y_pred,y_test))\n",
    "print('RMSE: {}'.format(loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f16e893-9669-4141-b96c-93149de8e9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_verify=pd.DataFrame(y_test.tolist(),columns=['Test'])\n",
    "data_predicted=pd.DataFrame(y_pred.tolist(),columns=['Prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "697af36c-4120-41b1-ae53-0e07e16ee8ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Diffrence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>130000.0</td>\n",
       "      <td>1029.289917</td>\n",
       "      <td>128970.710083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>138887.0</td>\n",
       "      <td>1164.394287</td>\n",
       "      <td>137722.605713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>175500.0</td>\n",
       "      <td>1106.706177</td>\n",
       "      <td>174393.293823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>195000.0</td>\n",
       "      <td>1079.853882</td>\n",
       "      <td>193920.146118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142500.0</td>\n",
       "      <td>1017.256897</td>\n",
       "      <td>141482.743103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Test   Prediction      Diffrence\n",
       "0  130000.0  1029.289917  128970.710083\n",
       "1  138887.0  1164.394287  137722.605713\n",
       "2  175500.0  1106.706177  174393.293823\n",
       "3  195000.0  1079.853882  193920.146118\n",
       "4  142500.0  1017.256897  141482.743103"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output=pd.concat([data_verify,data_predicted],axis=1)\n",
    "final_output['Diffrence']=final_output['Test']-final_output['Prediction']\n",
    "final_output.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "08ceb78c-d3db-495b-b8ae-67e0315689cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the model \n",
    "torch.save(model,'HousePrice.pt')  ## Saves the entire model (architecture + weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "380d2a39-6797-4d33-ad74-a034a406c79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'HouseWeights.pt')   # Saves only the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9cbf7cac-4bbb-441b-99d3-f9abfdcc492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_size=[(15, 8), (5, 3), (4, 2), (5, 3), (10, 5), (8, 4)]\n",
    "model1 = FeedForwardNN(embs_size, 5, [100, 50], 1, p=0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "084eedd7-5a8f-4993-b1e8-f677a1413a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.load_state_dict(torch.load('HouseWeights.pt')) # Loads only the weights into model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fec93e63-3f33-4307-869d-060389ecbe10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForwardNN(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(15, 8)\n",
       "    (1): Embedding(5, 3)\n",
       "    (2): Embedding(4, 2)\n",
       "    (3): Embedding(5, 3)\n",
       "    (4): Embedding(10, 5)\n",
       "    (5): Embedding(8, 4)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.4, inplace=False)\n",
       "  (bn_cont): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=30, out_features=100, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.4, inplace=False)\n",
       "    (8): Linear(in_features=50, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
